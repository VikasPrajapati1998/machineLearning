{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "741bd2ec-d767-4427-a84c-7bff2f93d897",
   "metadata": {},
   "source": [
    "### **Heuristic Approach:**\n",
    "1. Rule-based system: Define rules for answering questions based on keywords, synonyms, and context.\n",
    "2. Keyword extraction: Identify key phrases from questions and match with summary text.\n",
    "3. Pattern matching: Use regular expressions or string matching algorithms.\n",
    "\n",
    "\n",
    "### **Machine Learning Approach:**\n",
    "1. Text classification: Train models to classify questions into categories.\n",
    "2. Named Entity Recognition (NER): Identify entities in questions and summary.\n",
    "3. Question Answering (QA) models: Utilize models like BERT, RoBERTa, or XLNet.\n",
    "\n",
    "\n",
    "### **Steps:**\n",
    "1. Preprocess data: Tokenize, remove stop words, and lemmatize.\n",
    "2. Train models: Use scikit-learn, TensorFlow, or PyTorch.\n",
    "3. Integrate heuristic and ML approaches.\n",
    "\n",
    "\n",
    "### **Some popular ML models for QA:**\n",
    "1. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "2. RoBERTa (Robustly optimized BERT approach)\n",
    "3. XLNet (Extreme Language Modeling)\n",
    "4. DistilBERT (Distilled BERT)\n",
    "\n",
    "\n",
    "### **Tools and Libraries:**\n",
    "1. NLTK (Natural Language Toolkit)\n",
    "2. spaCy (Modern NLP library)\n",
    "3. scikit-learn (Machine learning library)\n",
    "4. TensorFlow or PyTorch (Deep learning libraries)\n",
    "5. Rasa or Dialogflow (Chatbot development frameworks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f396c8e-2a05-48b9-9153-276b53062506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "summary_text = [\n",
    "    'Initiate Trips table provides information about User login, Landing page, My Trip list, and Create new Trip.',\n",
    "    'Driver Manager Actions table provides information about actions such as adding a driver, editing driver details, adding new trucks.',\n",
    "    'Driver / Dispatcher table provides information about the Driver / Dispatcher List page, Truck Status, and Pre-Trip Checklist.',\n",
    "    'Dispatcher Actions table provides information about actions related to dispatchers, including the Dispatcher List page.',\n",
    "    'Compliance Assistant table provides information about the Compliance Assistant Trip List, Truck List, and checklists.',\n",
    "    'Compliance Supervisor table provides information about the Compliance Supervisor Trip List and checklists.',\n",
    "    'HV Assigner table provides information about Truck Assignment Board, Trip Detail, and Assign Load.',\n",
    "    'HV Assigner Manager table provides information about Trip Approval Status Board and Trip Detail.',\n",
    "    'Planning & Consolidation table provides information about the Segment Requester.',\n",
    "    \"JMS Coordinator table provides information about the JMS Coordinator List and the JMS id Entry form.\",\n",
    "    'Receiving table provides information about the HV Receiver Trip List and the HV Receiver Checklist.'\n",
    "]\n",
    "summary_questions = [\n",
    "    'What is the Initiate Trips table about?',\n",
    "    'What actions can the Driver Manager perform?',\n",
    "    'What does the Driver / Dispatcher table provide?',\n",
    "    'What actions does the Dispatcher Actions table cover?',\n",
    "    'What does the Compliance Assistant table provide?',\n",
    "    'What information does the Compliance Supervisor table contain?',\n",
    "    'What does the HV Assigner table cover?',\n",
    "    'What does the HV Assigner Manager table contain?',\n",
    "    'What information is in the Planning & Consolidation table?',\n",
    "    \"What does the JMS Coordinator table include?\",\n",
    "    'What information is in the Receiving table?'\n",
    "]\n",
    "\n",
    "# Questions answeres\n",
    "# Merged Dataset\n",
    "questions = [\n",
    "    {\n",
    "        'question': \"Hi\",\n",
    "        'answer': \"Hello! How can I assist you today?\"\n",
    "    },\n",
    "    {\n",
    "        'question': \"Hello\",\n",
    "        'answer': \"Hi there! What can I do for you?\"\n",
    "    },\n",
    "    {\n",
    "        'question': \"How are you?\",\n",
    "        'answer': \"I'm just a program, but I'm doing great! How about you?\"\n",
    "    },\n",
    "    {\n",
    "        'question': \"What is your name?\",\n",
    "        'answer': \"I'm your friendly chatbot!\"\n",
    "    },\n",
    "    {\n",
    "        'question': \"Tell me a joke.\",\n",
    "        'answer': \"Why don't scientists trust atoms? Because they make up everything!\"\n",
    "    },\n",
    "    {\n",
    "        'question': \"How fast are you?\",\n",
    "        'answer': \"I'm as fast as a program can be!\"\n",
    "    },\n",
    "    {\n",
    "        'question': \"Goodbye\",\n",
    "        'answer': \"Goodbye! Have a great day!\"\n",
    "    },\n",
    "    {\n",
    "        'question': \"Thank you\",\n",
    "        'answer': \"You're welcome!\"\n",
    "    },\n",
    "    {\n",
    "        'question': \"What can you do?\",\n",
    "        'answer': \"I can answer questions and chat with you!\"\n",
    "    },\n",
    "    {\n",
    "        'question': 'Can a trip request be edited after submitting it?',\n",
    "        'answer': \"Yes, both you and approver 1 from your segment can edit your fleet request before it has been approved. Go to 'My Trip Requests', select the request you want to edit, and click 'Edit'.\"\n",
    "    },\n",
    "    {\n",
    "        'question': 'Can I use data of a previously submitted trip in a new trip request?',\n",
    "        'answer': \"Yes. Go to 'My Trip Requests', select the request you want to copy data from and click “copy to new request”. You will be taken to the new trip request form with pre-filled data as per clicked Trip ID.\"\n",
    "    },\n",
    "    {\n",
    "        'question': 'How do I know if my trip has been approved?',\n",
    "        'answer': 'You will receive a notification once your request has been approved. You can also check the status in ‘Trip details page.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What should I do if my submitted trip is denied for approval?',\n",
    "        'answer': 'If your request is denied, you will receive a notification with the reason. You can resubmit new trip after inclusion of changes mentioned in rejection remarks.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Who do I contact for technical support?',\n",
    "        'answer': 'For technical support, please contact our project support team at - Anjani (before 6:30 PM (KSA)) - anjani@oges.co - +91 96 5465 80 01 & Sanskar (after 6:30 PM (KSA)) - sanskar.jain@oges.co - +91 98 2134 88 11'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How can I report application bugs or feature improvement requests in HVMS application?',\n",
    "        'answer': \"Application bugs or feature improvement requests by clicking the 'Feedback' button on the right side.\"\n",
    "    },\n",
    "    {\n",
    "        'question': 'Are there any training resources available for new users?',\n",
    "        'answer': 'Yes, for going through overall workflow of HVMS application, you can also download the application workflow help file from the “Help” section.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'User login',\n",
    "        'answer': '''\n",
    "                    Step 1- For login user must be connected NESR official network.\n",
    "                    Step 2- Open the link oges.nesr.com.\n",
    "                    Step 3 - NESR employee can login with his LDAP id.\n",
    "                    Step 4 - Click on the login button.\n",
    "                    Step 5 - Then you will be logged in to the website.\n",
    "                '''\n",
    "    },\n",
    "    {\n",
    "        'question': 'Landing page',\n",
    "        'answer': 'When landing on the NESR platform from different applications, to enter the HVMS application, click on the HVMS icon. Then you can access the HVMS platform.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'My Trip list',\n",
    "        'answer': '''\n",
    "                    Step 1 - For the Requester page, you will land on the Requester My Trip List page.\n",
    "                    Step 2 - On the Trip List page, the requester can see all their requested trips.\n",
    "                '''\n",
    "    },\n",
    "    {\n",
    "        'question': 'Create new Trip',\n",
    "        'answer': '''\n",
    "                    Step 1 - Click on the 'Create New Trip' text link, which is located on the menu bar. By clicking on 'Create New Trip', the requester will enter the Trip Request form.\n",
    "                    Step 2 - Every requester has a different segment. When the requester requests a trip, the trip goes under the requester's segment.\n",
    "                    Step 3 - In the Requested Date and Time field, the requester can select the date on which the trip will start. \n",
    "                    Step 4 - The requester can select the Trip Type (e.g., Tool movement) & Priority (e.g., normal) of the trip. \n",
    "                    Step 5 - The user can fill in all the trip details and click the \"Submit\" button. If the user wants to add more loads, they can click the \"Add Load\" button to include additional loads in a single trip.\n",
    "                    Step 6 - If the user does not have all the necessary data, they can save the request as a draft by clicking the \"Save as Draft\" button.\n",
    "                    Step 7 - Once the trip is created, it will appear on the list page. On this page, the user can edit the trip by clicking the \"Edit\" button.\n",
    "                    Step 8 - Once the user clicks the \"Edit\" button, they will land on the Edit Trip page. On this page, if the user makes any changes, they can click the \"Submit\" button. If they do not make any changes and wish to keep the existing data, they can click the \"Discard Changes\" button. By clicking either button, the user will return to the Trip List page.\n",
    "                    Step 9 - If the user does not want to proceed with the trip, they can cancel it by clicking the \"Cancel\" button. After clicking the \"Cancel\" button, a cancellation popup will appear. In the popup, the terms and conditions will be mentioned. If the user agrees with them, they can click the \"Cancel Trip\" button, and the trip will be canceled.\n",
    "                    Step 10 - On this page, only the trips that are saved as drafts will be displayed.\n",
    "                    Step 11 - To complete those trips, click the \"Edit\" button, fill in all the data, and then click the \"Submit\" button. After clicking the \"Submit\" button, the trip will disappear from this page and will appear on the Trip List page.\n",
    "                    Step 12 - On this page, all the requested trips are visible.\n",
    "                    Step 13 - On the Trip Detail page, all the requested trips are visible.\n",
    "                '''\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c8c92-6956-478b-bf93-60b895d5e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Preprocess questions for semantic similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([q['question'] for q in questions])\n",
    "\n",
    "\n",
    "# load dataset\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    summary_text = []\n",
    "    summary_questions = []\n",
    "    questions = []\n",
    "\n",
    "    section = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"# Summary\"):\n",
    "            section = \"summary\"\n",
    "            continue\n",
    "        elif line.startswith(\"# Questions Answers\"):\n",
    "            section = \"questions\"\n",
    "            continue\n",
    "        \n",
    "        if section == \"summary\":\n",
    "            # Handle summary lines\n",
    "            if line:\n",
    "                summary_questions.append(f\"What is the {line.split()[0]} table about?\")\n",
    "                summary_text.append(line)\n",
    "        elif section == \"questions\":\n",
    "            # Handle questions and answers\n",
    "            if line:\n",
    "                question, answer = line.split(' | ', 1)\n",
    "                questions.append({'question': question.strip(), 'answer': answer.strip()})\n",
    "\n",
    "    return summary_questions, summary_text, questions\n",
    "\n",
    "# Greeting Conversation\n",
    "def greeting_conversation(user_input: str) -> str:\n",
    "    # Create a TfidfVectorizer and fit it on the questions\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(normal_questions)\n",
    "    \n",
    "    # Train a simple classifier\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, np.arange(len(normal_questions)))\n",
    "    \n",
    "    user_input_vec = vectorizer.transform([user_input])\n",
    "    predicted_index = model.predict(user_input_vec)[0]\n",
    "    return responses[predicted_index]\n",
    "    \n",
    "\n",
    "# Train a simple classifier\n",
    "model = LogisticRegression()\n",
    "model.fit(X, np.arange(len(questions)))\n",
    "\n",
    "# Heuristic Approach\n",
    "def heuristic_approach(user_input: str) -> str:\n",
    "    tokens = word_tokenize(user_input, language='english', preserve_line=True)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for question in questions:\n",
    "        keywords = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(question['question'], language='english', preserve_line=True)]\n",
    "        match_count = sum(1 for token in tokens if lemmatizer.lemmatize(token.lower()) in keywords)\n",
    "\n",
    "        # Adjust the threshold value as needed\n",
    "        if match_count >= len(keywords) * 0.5:\n",
    "            return question['answer']\n",
    "\n",
    "    return None\n",
    "\n",
    "# Semantic Similarity Approach\n",
    "def semantic_similarity_approach(user_input: str):\n",
    "    user_input_vector = tfidf_vectorizer.transform([user_input])\n",
    "    similarities = cosine_similarity(user_input_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Find the index of the most similar question\n",
    "    answer_index = np.argmax(similarities)\n",
    "    return questions[answer_index]['answer']\n",
    "\n",
    "# Machine Learning Approach (if needed)\n",
    "def machine_learning_approach(user_input: str):\n",
    "    # Prepare the data\n",
    "    X = summary_questions  # Features\n",
    "    y = summary_text  # Labels\n",
    "    \n",
    "    # Create a TF-IDF Vectorizer and Multinomial Naive Bayes model\n",
    "    model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Function to get response based on user input\n",
    "    def get_response(user_input: str) -> str:\n",
    "        response = model.predict([user_input])\n",
    "        return response[0]\n",
    "    \n",
    "\n",
    "# Hybrid Approach\n",
    "def chatbot(user_input):\n",
    "    # Check with the heuristic approach first\n",
    "    answer = heuristic_approach(user_input)\n",
    "    \n",
    "    # If no answer found, check with semantic similarity\n",
    "    if answer is None:\n",
    "        answer = semantic_similarity_approach(user_input)\n",
    "\n",
    "    # If still no answer, use the machine learning approach\n",
    "    if answer is None:\n",
    "        answer = machine_learning_approach(user_input)\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Test Chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"Ask your question: \")\n",
    "        if user_input.lower() in ['none', '', 'thank you']:\n",
    "            break\n",
    "        answer = chatbot(user_input)\n",
    "        print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e35d17e4-11a0-48ba-9aa7-9074d07b0d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vikas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vikas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vikas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  hii\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  Compliance Assistant table provides information about the Compliance Assistant Trip List, Truck List, and checklists.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  what is hvms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  Receiving table provides information about the HV Receiver Trip List and the HV Receiver Checklist.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  How can I use hvms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  Driver Manager Actions table provides information about actions such as adding a driver, editing driver details, adding new trucks.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Thank you for using the chatbot.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Preprocess questions for semantic similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([q['question'] for q in questions])\n",
    "\n",
    "# Heuristic Approach\n",
    "def heuristic_approach(user_input: str) -> str:\n",
    "    tokens = word_tokenize(user_input, language='english', preserve_line=True)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for question in questions:\n",
    "        keywords = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(question['question'], language='english', preserve_line=True)]\n",
    "        match_count = sum(1 for token in tokens if lemmatizer.lemmatize(token.lower()) in keywords)\n",
    "        # Adjust the threshold value as needed\n",
    "        if match_count >= len(keywords) * 0.5:\n",
    "            return question['answer']\n",
    "    return None\n",
    "\n",
    "# Semantic Similarity Approach\n",
    "def semantic_similarity_approach(user_input: str):\n",
    "    user_input_vector = tfidf_vectorizer.transform([user_input])\n",
    "    similarities = cosine_similarity(user_input_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Find the index of the most similar question\n",
    "    max_similarity = np.max(similarities)\n",
    "    \n",
    "    # Set threshold: if less than 0.5 (50%), return None\n",
    "    if max_similarity < 0.5:\n",
    "        return None\n",
    "    \n",
    "    answer_index = np.argmax(similarities)\n",
    "    return questions[answer_index]['answer']\n",
    "\n",
    "# Machine Learning Approach for Summary\n",
    "def machine_learning_approach_summary(user_input: str):\n",
    "    # Prepare the data\n",
    "    X = summary_questions  # Features\n",
    "    y = summary_text      # Labels\n",
    "    # Create a TF-IDF Vectorizer and Multinomial Naive Bayes model\n",
    "    model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "    # Fit the model\n",
    "    model.fit(X, y)\n",
    "    # Predict the response based on user input\n",
    "    response = model.predict([user_input])\n",
    "    return response[0]\n",
    "\n",
    "# Hybrid Approach\n",
    "def chatbot(user_input):\n",
    "    # Check with the heuristic approach first\n",
    "    answer = heuristic_approach(user_input)\n",
    "    \n",
    "    # If no answer found, check with semantic similarity\n",
    "    if answer is None:\n",
    "        answer = semantic_similarity_approach(user_input)\n",
    "\n",
    "    # If still no answer, use the machine learning approach for summary questions\n",
    "    if answer is None:\n",
    "        answer = machine_learning_approach_summary(user_input)\n",
    "\n",
    "    if answer is None:\n",
    "        answer = \"Sorry, I am not able to answer.\"\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Test Chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    # summary_questions, summary_text, questions = load_dataset('nesr_chatbot_dataset.txt')\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in ['none', '', 'bye', 'exit']:\n",
    "            print(\"Chatbot: Thank you for using the chatbot.\")\n",
    "            break\n",
    "        answer = chatbot(user_input)\n",
    "        print(\"Chatbot: \", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a9ff6b9-419e-4163-b113-2af7e8bb8879",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7b493-a22e-4d70-bc0e-7ce38d4f1268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
