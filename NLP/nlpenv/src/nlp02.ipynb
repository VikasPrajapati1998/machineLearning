{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4656454-0ebb-4ee0-9507-ced389c4f313",
   "metadata": {},
   "source": [
    "# <b style=\"color:green\"> Text Preprocessing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cb1ff0c-1126-4fc1-8310-88f717c11937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a9e180-341e-4e77-9855-41799bef2455",
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = pd.read_csv(\"../data/IMDB_Dataset.csv\")\n",
    "adf.head()\n",
    "df = adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b0068c-cece-4511-982e-7193e1567341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42cd05af-284d-40ba-8795-87258902dd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d67971f-f631-4209-bcf7-d0f1ea3e4083",
   "metadata": {},
   "source": [
    "## <b style=\"color:red\">Lowercasing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e8e4e8-dce1-41c9-9a4f-ff0a361c4112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df['review'][3].lower()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4bfa314-78a0-44a8-a570-ceb46c96172b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b3160-3f70-46e3-ba96-32f3e6e15ab2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <b style=\"color:red\">Remove HTML Tags</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a98ea2-fb62-4050-89fc-450cfd3ac299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3711f7ed-e2ac-47ab-9d0f-52154b9b5851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(remove_html_tags)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d775343b-7e9a-47a5-9af0-4822efe44a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c280158-430a-43f7-a5a6-5c55d2830a3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <b style=\"color:red\">Remove URLs</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0481ba05-d59b-4cdf-89e3-542fbcbf3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3cc8cfa-f94e-494c-bad4-a1fe4f82472e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out my notebook \n",
      "Check out my notebook \n",
      "Google Search here \n",
      "For notebook click  to search check \n"
     ]
    }
   ],
   "source": [
    "url1 = \"Check out my notebook https://www.kaggle.com/campusx/notebook8223fc1labb\"\n",
    "url2 = \"Check out my notebook http://www.kaggle.com/campusx/notebook8223fc1labb\"\n",
    "url3 = \"Google Search here www.google.com\"\n",
    "url4 = \"For notebook click https://www.kaggle.com/campusx/notebook8223fc1labb to search check www.google.com\"\n",
    "\n",
    "url1 = remove_url(url1)\n",
    "print(url1)\n",
    "url2 = remove_url(url2)\n",
    "print(url2)\n",
    "url3 = remove_url(url3)\n",
    "print(url3)\n",
    "url4 = remove_url(url4)\n",
    "print(url4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e1386-3e73-400f-a158-f4e60a976413",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <b style=\"color:red\">Remove Punctuation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1779509-a914-4dcc-a86f-385df4f1fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f89906a5-0f21-4d8d-b1e8-37ca7e83a90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude = string.punctuation\n",
    "exclude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9795909-3b5e-4e08-8a06-937d5194f247",
   "metadata": {},
   "source": [
    "### **Method:1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e875755-75ab-496e-bac1-6e84d36981ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4231b427-dcf4-4311-99be-83ce0043c72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'String Width length height are use to take volume'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"String. Width, length, height, are use to take volume?\"\n",
    "text = remove_punc(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d54fda7-87e6-4ac7-901d-a0005210a735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String Width length height are use to take volume\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(remove_punc(text))\n",
    "t1 = time.time() - start\n",
    "print(t1)\n",
    "print(t1*50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18872a25-1dfb-48bd-ac43-894998bf5cdc",
   "metadata": {},
   "source": [
    "### **Method:2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9405af5-2f98-42cd-8a25-e6b2a39860ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69948f94-767f-4cf2-8517-f075dfb651c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String Width length height are use to take volume\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(remove_punc1(text))\n",
    "t2 = time.time() - start\n",
    "print(t2)\n",
    "print(t2*50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e2148c9-b3d9-49cf-bc70-cb874bca8b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation\n",
       "5   6      0  [2/2] huge fan fare and big talking before the...\n",
       "6   7      0   @user camping tomorrow @user @user @user @use..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/twitter-sentiment-hated-speech/train.csv\")\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3b1bf3e-7634-4151-a1b2-08c9afc1533a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo  '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1f367fc-8e85-4d38-91e2-ef93dd9b56d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>user when a father is dysfunctional and is so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>user user thanks for lyft credit i cant use ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>model   i love u take with u all the time in u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide society now    motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>22 huge fan fare and big talking before they l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>user camping tomorrow user user user user use...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   user when a father is dysfunctional and is so...\n",
       "1   2      0  user user thanks for lyft credit i cant use ca...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  model   i love u take with u all the time in u...\n",
       "4   5      0               factsguide society now    motivation\n",
       "5   6      0  22 huge fan fare and big talking before they l...\n",
       "6   7      0   user camping tomorrow user user user user use..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'] = df['tweet'].apply(remove_punc1)\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7478beb1-7e8a-4a8d-baaf-03077001a749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'22 huge fan fare and big talking before they leave chaos and pay disputes when they get there allshowandnogo  '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24522450-c3da-443c-8c4b-c2570ef8b5c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <b style=\"color:red\">Chat Word Treatment</b>\n",
    "- gn : good night\n",
    "- asap : as soon as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d274f012-d1d9-4445-92c8-144d4a9ed94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_sort_cut = '''AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It's Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great!\n",
    "G9=Genius\n",
    "IC=I See\n",
    "ICQ=I Seek you (also a chat program)\n",
    "ILU=I Love You\n",
    "IMHO=In My Honest/Humble Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple, Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My Ass Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The Ass\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "QPSA?=Que Pasa?\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "ASL=Age, Sex, Location\n",
    "THX=Thank You\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The Fuck\n",
    "WTG=Way To Go!\n",
    "WUF=Where Are You From?\n",
    "W8=Wait\n",
    "TFW=That feeling when. TFW internet slang often goes in a caption to an image.\n",
    "MFW=My face when\n",
    "MRW =My reaction when\n",
    "IFYP=I feel your pain\n",
    "LOL=Laughing out loud\n",
    "TNTL=Trying not to laugh\n",
    "JK=Just kidding\n",
    "IDC=I don’t care\n",
    "ILY=I love you\n",
    "IMU=I miss you\n",
    "ADIH=Another day in hell\n",
    "IDC=I don’t care\n",
    "ZZZ=Sleeping, bored, tired\n",
    "WYWH=Wish you were here\n",
    "TIME=Tears in my eyes\n",
    "BAE=Before anyone else\n",
    "FIMH=Forever in my heart\n",
    "BSAAW=Big smile and a wink\n",
    "BWL=Bursting with laughter\n",
    "LMAO=Laughing my ass off\n",
    "BFF=Best friends forever\n",
    "CSL=Can’t stop laughing'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16c9db75-c954-433c-9ddc-f4aa87376330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AFAIK=As Far As I Know',\n",
       " 'AFK=Away From Keyboard',\n",
       " 'ASAP=As Soon As Possible',\n",
       " 'ATK=At The Keyboard',\n",
       " 'ATM=At The Moment',\n",
       " 'A3=Anytime, Anywhere, Anyplace',\n",
       " 'BAK=Back At Keyboard',\n",
       " 'BBL=Be Back Later',\n",
       " 'BBS=Be Back Soon',\n",
       " 'BFN=Bye For Now',\n",
       " 'B4N=Bye For Now',\n",
       " 'BRB=Be Right Back',\n",
       " 'BRT=Be Right There',\n",
       " 'BTW=By The Way',\n",
       " 'B4=Before',\n",
       " 'B4N=Bye For Now',\n",
       " 'CU=See You',\n",
       " 'CUL8R=See You Later',\n",
       " 'CYA=See You',\n",
       " 'FAQ=Frequently Asked Questions',\n",
       " 'FC=Fingers Crossed',\n",
       " \"FWIW=For What It's Worth\",\n",
       " 'FYI=For Your Information',\n",
       " 'GAL=Get A Life',\n",
       " 'GG=Good Game',\n",
       " 'GN=Good Night',\n",
       " 'GMTA=Great Minds Think Alike',\n",
       " 'GR8=Great!',\n",
       " 'G9=Genius',\n",
       " 'IC=I See',\n",
       " 'ICQ=I Seek you (also a chat program)',\n",
       " 'ILU=I Love You',\n",
       " 'IMHO=In My Honest/Humble Opinion',\n",
       " 'IMO=In My Opinion',\n",
       " 'IOW=In Other Words',\n",
       " 'IRL=In Real Life',\n",
       " 'KISS=Keep It Simple, Stupid',\n",
       " 'LDR=Long Distance Relationship',\n",
       " 'LMAO=Laugh My Ass Off',\n",
       " 'LOL=Laughing Out Loud',\n",
       " 'LTNS=Long Time No See',\n",
       " 'L8R=Later',\n",
       " 'MTE=My Thoughts Exactly',\n",
       " 'M8=Mate',\n",
       " 'NRN=No Reply Necessary',\n",
       " 'OIC=Oh I See',\n",
       " 'PITA=Pain In The Ass',\n",
       " 'PRT=Party',\n",
       " 'PRW=Parents Are Watching',\n",
       " 'QPSA?=Que Pasa?',\n",
       " 'ROFL=Rolling On The Floor Laughing',\n",
       " 'ROFLOL=Rolling On The Floor Laughing Out Loud',\n",
       " 'ROTFLMAO=Rolling On The Floor Laughing My A.. Off',\n",
       " 'SK8=Skate',\n",
       " 'STATS=Your sex and age',\n",
       " 'ASL=Age, Sex, Location',\n",
       " 'THX=Thank You',\n",
       " 'TTFN=Ta-Ta For Now!',\n",
       " 'TTYL=Talk To You Later',\n",
       " 'U=You',\n",
       " 'U2=You Too',\n",
       " 'U4E=Yours For Ever',\n",
       " 'WB=Welcome Back',\n",
       " 'WTF=What The Fuck',\n",
       " 'WTG=Way To Go!',\n",
       " 'WUF=Where Are You From?',\n",
       " 'W8=Wait',\n",
       " 'TFW=That feeling when. TFW internet slang often goes in a caption to an image.',\n",
       " 'MFW=My face when',\n",
       " 'MRW =My reaction when',\n",
       " 'IFYP=I feel your pain',\n",
       " 'LOL=Laughing out loud',\n",
       " 'TNTL=Trying not to laugh',\n",
       " 'JK=Just kidding',\n",
       " 'IDC=I don’t care',\n",
       " 'ILY=I love you',\n",
       " 'IMU=I miss you',\n",
       " 'ADIH=Another day in hell',\n",
       " 'IDC=I don’t care',\n",
       " 'ZZZ=Sleeping, bored, tired',\n",
       " 'WYWH=Wish you were here',\n",
       " 'TIME=Tears in my eyes',\n",
       " 'BAE=Before anyone else',\n",
       " 'FIMH=Forever in my heart',\n",
       " 'BSAAW=Big smile and a wink',\n",
       " 'BWL=Bursting with laughter',\n",
       " 'LMAO=Laughing my ass off',\n",
       " 'BFF=Best friends forever',\n",
       " 'CSL=Can’t stop laughing']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_sort_cut = re.findall(\"\\S.*=*\\S\", chat_sort_cut)\n",
    "chat_sort_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b13a6831-4c14-4c00-b5f3-483a4900d5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AFAIK': 'As Far As I Know',\n",
       " 'AFK': 'Away From Keyboard',\n",
       " 'ASAP': 'As Soon As Possible',\n",
       " 'ATK': 'At The Keyboard',\n",
       " 'ATM': 'At The Moment',\n",
       " 'A3': 'Anytime, Anywhere, Anyplace',\n",
       " 'BAK': 'Back At Keyboard',\n",
       " 'BBL': 'Be Back Later',\n",
       " 'BBS': 'Be Back Soon',\n",
       " 'BFN': 'Bye For Now',\n",
       " 'B4N': 'Bye For Now',\n",
       " 'BRB': 'Be Right Back',\n",
       " 'BRT': 'Be Right There',\n",
       " 'BTW': 'By The Way',\n",
       " 'B4': 'Before',\n",
       " 'CU': 'See You',\n",
       " 'CUL8R': 'See You Later',\n",
       " 'CYA': 'See You',\n",
       " 'FAQ': 'Frequently Asked Questions',\n",
       " 'FC': 'Fingers Crossed',\n",
       " 'FWIW': \"For What It's Worth\",\n",
       " 'FYI': 'For Your Information',\n",
       " 'GAL': 'Get A Life',\n",
       " 'GG': 'Good Game',\n",
       " 'GN': 'Good Night',\n",
       " 'GMTA': 'Great Minds Think Alike',\n",
       " 'GR8': 'Great!',\n",
       " 'G9': 'Genius',\n",
       " 'IC': 'I See',\n",
       " 'ICQ': 'I Seek you (also a chat program)',\n",
       " 'ILU': 'I Love You',\n",
       " 'IMHO': 'In My Honest/Humble Opinion',\n",
       " 'IMO': 'In My Opinion',\n",
       " 'IOW': 'In Other Words',\n",
       " 'IRL': 'In Real Life',\n",
       " 'KISS': 'Keep It Simple, Stupid',\n",
       " 'LDR': 'Long Distance Relationship',\n",
       " 'LMAO': 'Laughing my ass off',\n",
       " 'LOL': 'Laughing out loud',\n",
       " 'LTNS': 'Long Time No See',\n",
       " 'L8R': 'Later',\n",
       " 'MTE': 'My Thoughts Exactly',\n",
       " 'M8': 'Mate',\n",
       " 'NRN': 'No Reply Necessary',\n",
       " 'OIC': 'Oh I See',\n",
       " 'PITA': 'Pain In The Ass',\n",
       " 'PRT': 'Party',\n",
       " 'PRW': 'Parents Are Watching',\n",
       " 'QPSA?': 'Que Pasa?',\n",
       " 'ROFL': 'Rolling On The Floor Laughing',\n",
       " 'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
       " 'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n",
       " 'SK8': 'Skate',\n",
       " 'STATS': 'Your sex and age',\n",
       " 'ASL': 'Age, Sex, Location',\n",
       " 'THX': 'Thank You',\n",
       " 'TTFN': 'Ta-Ta For Now!',\n",
       " 'TTYL': 'Talk To You Later',\n",
       " 'U': 'You',\n",
       " 'U2': 'You Too',\n",
       " 'U4E': 'Yours For Ever',\n",
       " 'WB': 'Welcome Back',\n",
       " 'WTF': 'What The Fuck',\n",
       " 'WTG': 'Way To Go!',\n",
       " 'WUF': 'Where Are You From?',\n",
       " 'W8': 'Wait',\n",
       " 'TFW': 'That feeling when. TFW internet slang often goes in a caption to an image.',\n",
       " 'MFW': 'My face when',\n",
       " 'MRW ': 'My reaction when',\n",
       " 'IFYP': 'I feel your pain',\n",
       " 'TNTL': 'Trying not to laugh',\n",
       " 'JK': 'Just kidding',\n",
       " 'IDC': 'I don’t care',\n",
       " 'ILY': 'I love you',\n",
       " 'IMU': 'I miss you',\n",
       " 'ADIH': 'Another day in hell',\n",
       " 'ZZZ': 'Sleeping, bored, tired',\n",
       " 'WYWH': 'Wish you were here',\n",
       " 'TIME': 'Tears in my eyes',\n",
       " 'BAE': 'Before anyone else',\n",
       " 'FIMH': 'Forever in my heart',\n",
       " 'BSAAW': 'Big smile and a wink',\n",
       " 'BWL': 'Bursting with laughter',\n",
       " 'BFF': 'Best friends forever',\n",
       " 'CSL': 'Can’t stop laughing'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_words = {}\n",
    "for chat in chat_sort_cut:\n",
    "    lst = chat.split('=')\n",
    "    chat_words[lst[0]] = lst[1]\n",
    "chat_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28af0a1d-07c4-43b2-ae45-17d3ce01c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c83afbf-8905-41ae-83ac-0364024ef3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In My Honest/Humble Opinion he the Best friends forever\n",
      "For Your Information I Love You Wait Talk To You Later I miss you\n"
     ]
    }
   ],
   "source": [
    "msg = chat_conversion('IMHO he the BFF')\n",
    "print(msg)\n",
    "msg = chat_conversion('FYI ILU W8 TTYL IMU')\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7efe3fe-9d03-42bd-90eb-b785b4af2f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    user when a father is dysfunctional and is so ...\n",
       "1    user user thanks for lyft credit i cant use ca...\n",
       "2                                  bihday your majesty\n",
       "3    model i love You take with You all the Tears i...\n",
       "4                    factsguide society now motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'] = df['tweet'].str.lower()\n",
    "df['tweet'] = df['tweet'].apply(remove_punc1)\n",
    "df['tweet'] = df['tweet'].apply(chat_conversion)\n",
    "df['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83257199-4925-433c-9507-f3a81b9ce196",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <b style=\"color:red\">Spelling Correction</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb28e0a7-1394-4453-bf7f-889dff3ba4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "# !conda install -c conda-forge textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85b7dc98-05be-4c04-b90c-ac97bebca8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please read the note book, and also like the notebook.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Please read the note book, ands also liake the notebook.\"\n",
    "# note book ----> notebook\n",
    "textblb = TextBlob(text)\n",
    "text = textblb.correct().string\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79239744-8564-42ce-9e98-bb4550241916",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <b style=\"color:red\">Remove Stop words</b>\n",
    "- a, the, of, are, my\n",
    "- Use `nltk` library\n",
    "- Do not remove stop words in `POS Tagging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10078fe7-9001-4727-b400-c0c8b909dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a09a850a-54a8-4280-8a8e-be7570eba991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "647bf36a-3a50-483a-a157-bbea1abaadcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bb0897f-aa11-440b-8364-bc865101dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "981e4501-4ae7-4369-9000-3ef24f7e23a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabiliy  all-time favorite movies,  story  selflessness, sacrifice  dedication   noble cause,  preachy boring.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"probabiliy my all-time favorite movies, a story of selflessness, sacrifice and dedication to a noble cause, but preachy boring.\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ef70b13-9a26-4b9c-84b7-0b60a7766963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     One    reviewers  mentioned   watching  1 Oz e...\n",
       "1     A wonderful little production. The filming tec...\n",
       "2     I thought    wonderful way  spend time    hot ...\n",
       "3     Basically there's  family   little boy (Jake) ...\n",
       "4     Petter Mattei's \"Love   Time  Money\"   visuall...\n",
       "                            ...                        \n",
       "95    Daniel Day-Lewis    versatile actor alive. Eng...\n",
       "96    My guess would    originally going    least tw...\n",
       "97    Well, I like  watch bad horror B-Movies, cause...\n",
       "98    This IS  worst movie I  ever seen,  well as,  ...\n",
       "99    I    Mario fan   long  I  remember, I   fond m...\n",
       "Name: review, Length: 100, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/IMDB_Dataset.csv\", nrows=100)\n",
    "df['review'] = df['review'].apply(remove_html_tags)\n",
    "df['review'] = df['review'].apply(remove_stopwords)\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42a6646-0004-46a0-a7d8-2f863163f495",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <b style=\"color:red\">Handling Emojis</b>\n",
    "- Remove with textual data\n",
    "- Replace with emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30d0fa-97ce-4381-aca8-d079b5f8eecd",
   "metadata": {},
   "source": [
    "### **To Remove Emojis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0f6c108-8880-4103-b79b-54e8c1038267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love Python! . This movie is amazing! . Just finished my workout ‍\n"
     ]
    }
   ],
   "source": [
    "# To remove emojis\n",
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emotions\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"  \n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Example usage:\n",
    "text_with_emojis = \"I love Python! 😍🐍. This movie is amazing! 🍿🎬. Just finished my workout 💪🏋️‍♀️\"\n",
    "clean_text = remove_emoji(text_with_emojis)\n",
    "print(clean_text)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff223fc-3e71-4be2-a77e-feb5bec32e5d",
   "metadata": {},
   "source": [
    "### **Replace With emojis Meaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b826939-61a0-411e-8e86-68b9df8dffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushpa name sun ke :cherry_blossom: smjha kya :fire: hai main.\n",
      "You are not a friend, you are a :snake:.\n"
     ]
    }
   ],
   "source": [
    "# Replace with meaning\n",
    "import emoji\n",
    "print(emoji.demojize('Pushpa name sun ke 🌸 smjha kya 🔥 hai main.'))\n",
    "print(emoji.demojize( \"You are not a friend, you are a 🐍.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f12f6-d5bc-4e2a-a3ff-ecc5dbf6db07",
   "metadata": {},
   "source": [
    "## <b style=\"color:red\">Tokenization</b>\n",
    "- I am an Indian.   >-------->>>     \"I\" \"am\", \"an\", \"India\"\n",
    "- Word level tokenization. Sentence level tokenization.\n",
    "- __Why tokenization?__\n",
    "- text classification : Costumer chat : Support, Sales\n",
    "  - Get number of unique words.\n",
    "  - eg. : I am new in new delhi.  >-------->>> I:1, am:2, new:3, in:4, delhi:5\n",
    "- __Problem in tokenization.__\n",
    "  - _Prefix_ : Characters at the begining. : `$(\"` : eg. $10\n",
    "  - _Suffix_ : Characters at the end. : `km),.!\"` : eg. 23Km\n",
    "  - _Infix_ : Characters in between. : `- -- / ...` : eg. New-York\n",
    "  - _Exception_ : Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied. : eg. Let's, U.S.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971218bb-d3cb-4437-8e64-e7a5ebae7169",
   "metadata": {},
   "source": [
    "### 1. Using the split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "755b0bf3-d6ee-4259-8df0-5e8cc36e98df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'going', 'to', 'delhi']\n",
      "['I am going to delhi', ' I will stay there for 3 days', \" Let's hope the trip to be great\"]\n",
      "['I', 'am', 'going', 'to', 'delhi!']\n",
      "['Where do think I should go? I have 3 day holiday']\n"
     ]
    }
   ],
   "source": [
    "# word tokenization\n",
    "sent1 = 'I am going to delhi'\n",
    "sent1 = sent1.split()\n",
    "print(sent1)\n",
    "\n",
    "# sentence tokenization\n",
    "sent2 = 'I am going to delhi. I will stay there for 3 days. Let\\'s hope the trip to be great'\n",
    "sent2 = sent2.split('.')\n",
    "print(sent2)\n",
    "\n",
    "# problem with split function\n",
    "sent3 = 'I am going to delhi!'\n",
    "sent3 = sent3.split()\n",
    "print(sent3)\n",
    "\n",
    "sent4 = 'Where do think I should go? I have 3 day holiday'\n",
    "sent4 = sent4.split('.')\n",
    "print(sent4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6211d5d-d9ac-408c-aa65-b0f4a6d30f6a",
   "metadata": {},
   "source": [
    "### 2. Using Regular Expression (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c01c7f4-0d9b-4f03-8f53-50134563b249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'going', 'to', 'delhi']\n",
      "\n",
      "['Lorem Ipsum is simply dummy text of the printing and typesetting industry', \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s\", 'when an unknown printer took a galley of type and scrambled it to make a type specimen book']\n",
      "\n",
      "Lorem Ipsum is simply dummy text of the printing and typesetting industry\n",
      "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s\n",
      "when an unknown printer took a galley of type and scrambled it to make a type specimen book\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sent11 = \"I am going to delhi!\"\n",
    "tokens = re.findall(\"[\\w']+\", sent11)\n",
    "print(tokens, end=\"\\n\\n\")\n",
    "\n",
    "sent12 = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sent12 = re.compile('[.,!?]').split(sent12)\n",
    "# sent12 = re.split(r'[.!?]', sent12)\n",
    "sent12 = [sent.strip() for sent in sent12 if sent.strip()]\n",
    "print(sent12, end=\"\\n\\n\")\n",
    "for x in sent12:\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a99e77-45f3-4306-9d51-70783cb75fce",
   "metadata": {},
   "source": [
    "### 3. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f157e2a-f3ff-46c0-a90b-7b73e32cdd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fde2fb42-006f-46b4-992b-a9db8278b595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'going', 'to', 'visit', 'delhi', '!']\n",
      "\n",
      "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?', \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]\n",
      "Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
      "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
      "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\n",
      "\n",
      "['I', 'have', 'a', 'Ph.D', 'in', 'A.I', '.']\n",
      "['We', \"'re\", 'here', 'to', 'help', '!', 'mail', 'us', 'at', 'nashik', '@', 'gmail.com']\n",
      "['A', '5km', 'ride', 'cost', '$', '10.50', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# word_tokenize\n",
    "sentence1 = 'I am going to visit delhi!'\n",
    "sentence1 = word_tokenize(sentence1)\n",
    "print(sentence1, end=\"\\n\\n\")\n",
    "\n",
    "sentence2 = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sentence2 = sent_tokenize(sentence2)\n",
    "print(sentence2, end=\"\\n\")\n",
    "for x in sentence2:\n",
    "    print(x)\n",
    "print()\n",
    "\n",
    "s1 = 'I have a Ph.D in A.I.'\n",
    "s2 = \"We're here to help! mail us at nashik@gmail.com\"\n",
    "s3 = 'A 5km ride cost $10.50.'\n",
    "\n",
    "s1 = word_tokenize(s1)\n",
    "print(s1)\n",
    "s2 = word_tokenize(s2)\n",
    "print(s2)\n",
    "s3 = word_tokenize(s3)\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1497ff-565a-4121-8b8e-aba7023e6872",
   "metadata": {},
   "source": [
    "### 4. Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb29e0b4-d4ab-426b-b3bc-4abd664f8751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "249c038f-c202-44e3-8964-7ba78533baf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\thave\ta\tPh\t.\tD\tin\tA.I.\t\n",
      "We\t're\there\tto\thelp\t!\tmail\tus\tat\tnashik2023@gmail.com\t\n",
      "A\t5\tkm\tride\tcost\t$\t10.50\t.\t\n",
      "I\tam\tgoing\tto\tvisit\tdelhi\t!\t\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "s1 = 'I have a Ph.D in A.I.'\n",
    "s2 = \"We're here to help! mail us at nashik2023@gmail.com\"\n",
    "s3 = 'A 5km ride cost $10.50.'\n",
    "s4 = 'I am going to visit delhi!'\n",
    "\n",
    "doc1 = nlp(s1)\n",
    "for token in doc1:\n",
    "    print(token, end=\"\\t\")\n",
    "print()\n",
    "doc2 = nlp(s2)\n",
    "for token in doc2:\n",
    "    print(token, end=\"\\t\")\n",
    "print()\n",
    "doc3 = nlp(s3)\n",
    "for token in doc3:\n",
    "    print(token, end=\"\\t\")\n",
    "print()\n",
    "doc4 = nlp(s4)\n",
    "for token in doc4:\n",
    "    print(token, end=\"\\t\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75a70d-42f2-4aa6-9a9d-8949e3718239",
   "metadata": {},
   "source": [
    "## <b style=\"color:red\">Stemming & Lemmatization</b>\n",
    "- __Inflection__ : In grammer, inflection is the modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood.\n",
    "- A little change in word to change the meaning. eg.\n",
    "  - `walk >--->>> walk, walking, walked walks`\n",
    "  - `do >--->>> does, undo, undoable`\n",
    "- <b style=\"color:blue\">__Stemming__</b> : Stemming is the process of reducing _inflection_ in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\n",
    "  - walking >--->>> walk\n",
    "  - It is text preprocessing technique.\n",
    "  - Use in IR Systems : Information Retrival System\n",
    "  - For stemming we use `nltk` library.\n",
    "  - NLTK -\n",
    "    - Porter Stemming\n",
    "    - Snow Ball Stemmer\n",
    "- Some times the output of __stemming__ will not be a english word. To solve this problem we use __lemmatization__. Work of __lemmatization__ is same as __stemming__.\n",
    "- Stemming is faster than lemmatization.\n",
    "- <b style=\"color:blue\">__Lemmatization__</b> : Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called __Lemma__. A lemma (plural lemmas or lemmata) is the canonical form, dictionary from, or citation form of a set of words.\n",
    "- Stemming use algoriths to eliminate prefix, infix or postfix. Where Lemmatization use a dictionary to search the word. We use __WordNet Lemmatizer__ dictionary, which is a __Lexical Dictionary__. Stemming is faster than Lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f934f2-eb10-4ef2-8ad7-52b18ec122c9",
   "metadata": {},
   "source": [
    "### 1. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6cfe8868-49c0-4431-93d2-034939ed0e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "969263cb-90b6-4bb9-861a-7c89aece442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk walk walk walk\n",
      "i am go to read a book\n"
     ]
    }
   ],
   "source": [
    "sample = \"walk walks walking walked\"\n",
    "sample = stem_words(sample)\n",
    "print(sample)\n",
    "\n",
    "sample = \"I am going to read a books\"\n",
    "sample = stem_words(sample)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0706e-3af6-4d9d-9b55-4b10c7631af7",
   "metadata": {},
   "source": [
    "### 2. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9f05a052-60ca-4329-b7e8-8a8bd47de6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1218805-68b6-4ca5-a65c-aac3cbdcf49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "some                some                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habbit              habbit              \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at some time. He has bad habbit of swimming after playing long hours in Sun.\"\n",
    "punctuations = \"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\", \"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print(\"{0:20}{1:20}\".format(word, wordnet_lemmatizer.lemmatize(word, pos='v'))) \n",
    "    # pov='v' // pos:part of speech, v:verb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a90b28c-81cd-4edd-ab00-3c3993dddfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
