{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b912a3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-27T04:19:51.361325Z",
     "iopub.status.busy": "2024-08-27T04:19:51.361033Z",
     "iopub.status.idle": "2024-08-27T04:20:33.504657Z",
     "shell.execute_reply": "2024-08-27T04:20:33.503584Z"
    },
    "papermill": {
     "duration": 42.152754,
     "end_time": "2024-08-27T04:20:33.506566",
     "exception": false,
     "start_time": "2024-08-27T04:19:51.353812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-361-gc5ffbbf1 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete ‚úÖ (4 CPUs, 31.4 GB RAM, 5845.9/8062.4 GB disk)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5  # clone\n",
    "%cd yolov5\n",
    "%pip install -qr requirements.txt  # install\n",
    "\n",
    "import torch\n",
    "import utils\n",
    "display = utils.notebook_init()  # checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4166604",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T04:20:33.520041Z",
     "iopub.status.busy": "2024-08-27T04:20:33.519652Z",
     "iopub.status.idle": "2024-08-27T04:21:35.937657Z",
     "shell.execute_reply": "2024-08-27T04:21:35.936500Z"
    },
    "papermill": {
     "duration": 62.427442,
     "end_time": "2024-08-27T04:21:35.940104",
     "exception": false,
     "start_time": "2024-08-27T04:20:33.512662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 780M/780M [00:53<00:00, 15.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download COCO val\n",
    "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n",
    "!unzip -q tmp.zip -d ../datasets && rm tmp.zip  # unzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332e91fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T04:21:35.996954Z",
     "iopub.status.busy": "2024-08-27T04:21:35.996632Z",
     "iopub.status.idle": "2024-08-27T04:26:16.908481Z",
     "shell.execute_reply": "2024-08-27T04:26:16.907254Z"
    },
    "papermill": {
     "duration": 280.94346,
     "end_time": "2024-08-27T04:26:16.910897",
     "exception": false,
     "start_time": "2024-08-27T04:21:35.967437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/kaggle/working/yolov5/data/coco.yaml, weights=['yolov5n.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=0, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\r\n",
      "YOLOv5 üöÄ v7.0-361-gc5ffbbf1 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\r\n",
      "\r\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt to yolov5n.pt...\r\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.87M/3.87M [00:00<00:00, 218MB/s]\r\n",
      "\r\n",
      "Fusing layers... \r\n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\r\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/datasets/coco/val2017... 4952 images, 48 backgroun\u001b[0m\r\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/datasets/coco/val2017.cache\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       5000      36335      0.574      0.432      0.456      0.277\r\n",
      "Speed: 0.1ms pre-process, 1.7ms inference, 2.2ms NMS per image at shape (32, 3, 640, 640)\r\n",
      "\r\n",
      "Evaluating pycocotools mAP... saving runs/val/exp/yolov5n_predictions.json...\r\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['pycocotools>=2.0.6'] not found, attempting AutoUpdate...\r\n",
      "Collecting pycocotools>=2.0.6\r\n",
      "  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\r\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>=2.0.6) (3.7.5)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools>=2.0.6) (1.26.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (1.4.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (21.3)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (10.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.6) (1.16.0)\r\n",
      "Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m427.8/427.8 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pycocotools\r\n",
      "Successfully installed pycocotools-2.0.8\r\n",
      "\r\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 11.7s, installed 1 package: ['pycocotools>=2.0.6']\r\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\r\n",
      "\r\n",
      "loading annotations into memory...\r\n",
      "Done (t=0.67s)\r\n",
      "creating index...\r\n",
      "index created!\r\n",
      "Loading and preparing results...\r\n",
      "DONE (t=8.50s)\r\n",
      "creating index...\r\n",
      "index created!\r\n",
      "Running per image evaluation...\r\n",
      "Evaluate annotation type *bbox*\r\n",
      "DONE (t=125.91s).\r\n",
      "Accumulating evaluation results...\r\n",
      "DONE (t=27.09s).\r\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.281\r\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.462\r\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.292\r\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.141\r\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.322\r\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.367\r\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.252\r\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.433\r\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\r\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.279\r\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.541\r\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.632\r\n",
      "Results saved to \u001b[1mruns/val/exp\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Validate YOLOv5n on COCO val\n",
    "!python val.py --weights yolov5n.pt --data coco.yaml --img 640 --device 0 --task val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4304435",
   "metadata": {
    "papermill": {
     "duration": 0.041316,
     "end_time": "2024-08-27T04:26:16.994356",
     "exception": false,
     "start_time": "2024-08-27T04:26:16.953040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **ONNX FP32**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e998c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T04:26:17.079662Z",
     "iopub.status.busy": "2024-08-27T04:26:17.078769Z",
     "iopub.status.idle": "2024-08-27T04:26:27.846430Z",
     "shell.execute_reply": "2024-08-27T04:26:27.845160Z"
    },
    "papermill": {
     "duration": 10.813545,
     "end_time": "2024-08-27T04:26:27.849194",
     "exception": false,
     "start_time": "2024-08-27T04:26:17.035649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['/kaggle/working/yolov5/yolov5n.pt'], imgsz=[640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, per_tensor=False, dynamic=False, simplify=False, mlmodel=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\r\n",
      "YOLOv5 üöÄ v7.0-361-gc5ffbbf1 Python-3.10.13 torch-2.1.2 CPU\r\n",
      "\r\n",
      "Fusing layers... \r\n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\r\n",
      "\r\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from /kaggle/working/yolov5/yolov5n.pt with output shape (1, 25200, 85) (3.9 MB)\r\n",
      "\r\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0...\r\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 0.8s, saved as /kaggle/working/yolov5/yolov5n.onnx (7.6 MB)\r\n",
      "\r\n",
      "Export complete (1.3s)\r\n",
      "Results saved to \u001b[1m/kaggle/working/yolov5\u001b[0m\r\n",
      "Detect:          python detect.py --weights /kaggle/working/yolov5/yolov5n.onnx \r\n",
      "Validate:        python val.py --weights /kaggle/working/yolov5/yolov5n.onnx \r\n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', '/kaggle/working/yolov5/yolov5n.onnx')  \r\n",
      "Visualize:       https://netron.app\r\n"
     ]
    }
   ],
   "source": [
    "# convert pytorch model to onnx fp32 model\n",
    "!python export.py --weights /kaggle/working/yolov5/yolov5n.pt --img 640 --batch 1 --include onnx\n",
    "!mv /kaggle/working/yolov5/yolov5n.onnx /kaggle/working/yolov5/yolov5n_fp32.onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b41c16d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T04:26:27.940731Z",
     "iopub.status.busy": "2024-08-27T04:26:27.940316Z",
     "iopub.status.idle": "2024-08-27T04:35:56.381488Z",
     "shell.execute_reply": "2024-08-27T04:35:56.380349Z"
    },
    "papermill": {
     "duration": 568.487793,
     "end_time": "2024-08-27T04:35:56.383796",
     "exception": false,
     "start_time": "2024-08-27T04:26:27.896003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/kaggle/working/yolov5/data/coco.yaml, weights=['/kaggle/working/yolov5/yolov5n_fp32.onnx'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=0, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\r\n",
      "YOLOv5 üöÄ v7.0-361-gc5ffbbf1 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\r\n",
      "\r\n",
      "Loading /kaggle/working/yolov5/yolov5n_fp32.onnx for ONNX Runtime inference...\r\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnxruntime-gpu'] not found, attempting AutoUpdate...\r\n",
      "Collecting onnxruntime-gpu\r\n",
      "  Downloading onnxruntime_gpu-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\r\n",
      "Collecting coloredlogs (from onnxruntime-gpu)\r\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime-gpu) (23.5.26)\r\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from onnxruntime-gpu) (1.26.4)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from onnxruntime-gpu) (21.3)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime-gpu) (3.20.3)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime-gpu) (1.12)\r\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\r\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->onnxruntime-gpu) (3.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime-gpu) (1.3.0)\r\n",
      "Downloading onnxruntime_gpu-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (223.1 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m223.1/223.1 MB\u001b[0m \u001b[31m181.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m171.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m218.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime-gpu\r\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-gpu-1.19.0\r\n",
      "\r\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 18.5s, installed 1 package: ['onnxruntime-gpu']\r\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\r\n",
      "\r\n",
      "\u001b[1;31m2024-08-27 04:26:53.834006735 [E:onnxruntime:Default, provider_bridge_ort.cc:1992 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1637 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.9: cannot open shared object file: No such file or directory\r\n",
      "\u001b[m\r\n",
      "\u001b[0;93m2024-08-27 04:26:53.834037083 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:965 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.\u001b[m\r\n",
      "Forcing --batch-size 1 square inference (1,3,640,640) for non-PyTorch models\r\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/datasets/coco/val2017.cache... 4952 images, 48 bac\u001b[0m\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       5000      36335      0.575      0.427      0.455      0.277\r\n",
      "Speed: 0.2ms pre-process, 50.6ms inference, 2.6ms NMS per image at shape (1, 3, 640, 640)\r\n",
      "\r\n",
      "Evaluating pycocotools mAP... saving runs/val/exp2/yolov5n_fp32_predictions.json...\r\n",
      "loading annotations into memory...\r\n",
      "Done (t=0.65s)\r\n",
      "creating index...\r\n",
      "index created!\r\n",
      "Loading and preparing results...\r\n",
      "DONE (t=8.76s)\r\n",
      "creating index...\r\n",
      "index created!\r\n",
      "Running per image evaluation...\r\n",
      "Evaluate annotation type *bbox*\r\n",
      "DONE (t=125.73s).\r\n",
      "Accumulating evaluation results...\r\n",
      "DONE (t=27.48s).\r\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.281\r\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.461\r\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.292\r\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.140\r\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.321\r\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.368\r\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.252\r\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.433\r\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\r\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.282\r\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539\r\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.639\r\n",
      "Results saved to \u001b[1mruns/val/exp2\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Validation yolov5n.onnx on COCO val\n",
    "!python val.py --weights /kaggle/working/yolov5/yolov5n_fp32.onnx --data coco.yaml --img 640 --device 0 --task val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cb5fbc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T04:35:56.861907Z",
     "iopub.status.busy": "2024-08-27T04:35:56.861137Z",
     "iopub.status.idle": "2024-08-27T04:36:22.775666Z",
     "shell.execute_reply": "2024-08-27T04:36:22.774549Z"
    },
    "papermill": {
     "duration": 26.15617,
     "end_time": "2024-08-27T04:36:22.778011",
     "exception": false,
     "start_time": "2024-08-27T04:35:56.621841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime\r\n",
      "  Downloading onnxruntime-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\r\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (15.0.1)\r\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (23.5.26)\r\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (1.26.4)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (21.3)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (3.20.3)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (1.12)\r\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->onnxruntime) (10.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->onnxruntime) (3.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime) (1.3.0)\r\n",
      "Downloading onnxruntime-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: onnxruntime\r\n",
      "Successfully installed onnxruntime-1.19.0\r\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.16)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from timm) (2.1.2)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.2)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0.1)\r\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.20.3)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.2.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.9.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (21.3)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->timm) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.1.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (10.4.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub->timm) (3.1.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime\n",
    "!pip install timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8775f80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T04:36:23.300189Z",
     "iopub.status.busy": "2024-08-27T04:36:23.299132Z",
     "iopub.status.idle": "2024-08-27T04:36:24.172225Z",
     "shell.execute_reply": "2024-08-27T04:36:24.170773Z"
    },
    "papermill": {
     "duration": 1.114702,
     "end_time": "2024-08-27T04:36:24.173889",
     "exception": true,
     "start_time": "2024-08-27T04:36:23.059187",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "INT4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Quantization Class\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01monnxruntime\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mort\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnxruntime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantization\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mOnnxStaticQuantization\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/onnxruntime/quantization/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcalibrate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     CalibraterBase,\n\u001b[1;32m      3\u001b[0m     CalibrationDataReader,\n\u001b[1;32m      4\u001b[0m     CalibrationMethod,\n\u001b[1;32m      5\u001b[0m     MinMaxCalibrater,\n\u001b[1;32m      6\u001b[0m     create_calibrator,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqdq_quantizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QDQQuantizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquant_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuantFormat, QuantType, write_calibration_table  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/onnxruntime/quantization/calibrate.py:22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelProto, TensorProto, helper, numpy_helper\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01monnxruntime\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquant_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_plot, load_model_with_shape_infer, smooth_distribution\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrel_entr\u001b[39m(pk: np\u001b[38;5;241m.\u001b[39mndarray, qk: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    See https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.rel_entr.html#scipy.special.rel_entr.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Python implementation.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/onnxruntime/quantization/quant_utils.py:145\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m()  \u001b[38;5;66;03m# noqa: B904\u001b[39;00m\n\u001b[1;32m    139\u001b[0m ONNX_TYPE_TO_NP_TYPE \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    140\u001b[0m     onnx_proto\u001b[38;5;241m.\u001b[39mTensorProto\u001b[38;5;241m.\u001b[39mINT8: numpy\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    141\u001b[0m     onnx_proto\u001b[38;5;241m.\u001b[39mTensorProto\u001b[38;5;241m.\u001b[39mUINT8: numpy\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    142\u001b[0m     onnx_proto\u001b[38;5;241m.\u001b[39mTensorProto\u001b[38;5;241m.\u001b[39mINT16: numpy\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    143\u001b[0m     onnx_proto\u001b[38;5;241m.\u001b[39mTensorProto\u001b[38;5;241m.\u001b[39mUINT16: numpy\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint16\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    144\u001b[0m     onnx_proto\u001b[38;5;241m.\u001b[39mTensorProto\u001b[38;5;241m.\u001b[39mFLOAT8E4M3FN: float8e4m3fn,\n\u001b[0;32m--> 145\u001b[0m     \u001b[43monnx_proto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorProto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINT4\u001b[49m: int4,  \u001b[38;5;66;03m# base_dtype is np.int8\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     onnx_proto\u001b[38;5;241m.\u001b[39mTensorProto\u001b[38;5;241m.\u001b[39mUINT4: uint4,  \u001b[38;5;66;03m# base_dtype is np.uint8\u001b[39;00m\n\u001b[1;32m    147\u001b[0m }\n\u001b[1;32m    149\u001b[0m ONNX_INT_TYPE_RANGE \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    150\u001b[0m     onnx_proto\u001b[38;5;241m.\u001b[39mTensorProto\u001b[38;5;241m.\u001b[39mUINT8: (numpy\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39muint8), numpy\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m255\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39muint8)),\n\u001b[1;32m    151\u001b[0m     onnx_proto\u001b[38;5;241m.\u001b[39mTensorProto\u001b[38;5;241m.\u001b[39mINT8: (numpy\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m128\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mint8), numpy\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m127\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mint8)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m     onnx_proto\u001b[38;5;241m.\u001b[39mTensorProto\u001b[38;5;241m.\u001b[39mINT4: (numpy\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m8\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mint4), numpy\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m7\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mint4)),\n\u001b[1;32m    156\u001b[0m }\n\u001b[1;32m    158\u001b[0m ONNX_INT_TYPE_SYMMETRIC_RANGE \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    159\u001b[0m     onnx_proto\u001b[38;5;241m.\u001b[39mTensorProto\u001b[38;5;241m.\u001b[39mINT8: (numpy\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m127\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mint8), numpy\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m127\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mint8)),\n\u001b[1;32m    160\u001b[0m     onnx_proto\u001b[38;5;241m.\u001b[39mTensorProto\u001b[38;5;241m.\u001b[39mINT16: (numpy\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m32767\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mint16), numpy\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m32767\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mint16)),\n\u001b[1;32m    161\u001b[0m }\n",
      "\u001b[0;31mAttributeError\u001b[0m: INT4"
     ]
    }
   ],
   "source": [
    "# Quantization Class\n",
    "import onnxruntime as ort\n",
    "from onnxruntime import quantization\n",
    "import timm\n",
    "\n",
    "class OnnxStaticQuantization:\n",
    "    def __init__(self) -> None:\n",
    "        self.enum_data = None\n",
    "        self.calibration_technique = {\n",
    "            \"MinMax\": ort.quantization.calibrate.CalibrationMethod.MinMax,\n",
    "            \"Entropy\": ort.quantization.calibrate.CalibrationMethod.Entropy,\n",
    "            \"Percentile\": ort.quantization.calibrate.CalibrationMethod.Percentile,\n",
    "            \"Distribution\": ort.quantization.calibrate.CalibrationMethod.Distribution\n",
    "        }\n",
    "        \n",
    "    def get_next(self, EP_list = ['CPUExecutionProvider']):\n",
    "        if self.enum_data is None:\n",
    "            session = ort.InferenceSession(self.fp32_onnx_path, providers=EP_list)\n",
    "            input_name = session.get_inputs()[0].name\n",
    "            calib_list = []\n",
    "            count = 0\n",
    "            for nhwc_data, _ in self.calibration_loader:\n",
    "                nhwc_data=nhwc_data.cpu()\n",
    "                calib_list.append({input_name: nhwc_data.numpy()})\n",
    "                if self.sample == count: break\n",
    "                count = count + 1\n",
    "            self.enum_data = iter(calib_list)\n",
    "        return next(self.enum_data, None)\n",
    "    \n",
    "    def quantization(self, fp32_onnx_path, future_int8_onnx_path, calib_method, calibration_loader, sample=100):\n",
    "        self.sample = sample\n",
    "        self.calibration_loader = calibration_loader\n",
    "        _ = ort.quantization.quantize_static(\n",
    "                model_input=fp32_onnx_path,\n",
    "                model_output=future_int8_onnx_path,\n",
    "                activation_type=ort.quantization.QuantType.QInt16, \n",
    "                weight_type=ort.quantization.QuantType.QInt16,\n",
    "                calibrate_method=self.calibration_technique[calib_method],\n",
    "                calibration_data_reader=self\n",
    "            )\n",
    "        return self\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cdec9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T13:00:54.350001Z",
     "iopub.status.busy": "2024-03-17T13:00:54.349143Z",
     "iopub.status.idle": "2024-03-17T13:00:54.516153Z",
     "shell.execute_reply": "2024-03-17T13:00:54.514870Z",
     "shell.execute_reply.started": "2024-03-17T13:00:54.349968Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# Perform the quantization\n",
    "val_dataset = timm.data.ImageDataset('/kaggle/working/datasets/coco/images/val2017')\n",
    "val_loader = timm.data.create_loader(val_dataset, (1,3,640,640), 1)\n",
    "module = OnnxStaticQuantization()\n",
    "module.fp32_onnx_path = \"/kaggle/working/yolov5/yolov5n_fp32.onnx\"\n",
    "\n",
    "# method=Distribution, calibration_number=100\n",
    "module.quantization(\n",
    "    fp32_onnx_path=\"yolov5n_fp32.onnx\",\n",
    "    future_int8_onnx_path=\"yolov5n_fp32_MinMax.onnx\",\n",
    "    calib_method=\"MinMax\",\n",
    "    calibration_loader=val_loader,\n",
    "    sample=1000   # calibration number\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f079dd16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T12:38:39.017106Z",
     "iopub.status.busy": "2024-03-17T12:38:39.016669Z",
     "iopub.status.idle": "2024-03-17T12:38:48.558213Z",
     "shell.execute_reply": "2024-03-17T12:38:48.556942Z",
     "shell.execute_reply.started": "2024-03-17T12:38:39.017060Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validation yolov5n_fp32_MinMax.onnx on COCO val\n",
    "!python val.py --weights /kaggle/working/yolov5/yolov5n_fp32_MinMax.onnx --data coco.yaml --img 640 --device 0 --task val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e526040",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## **Different Calibration Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e0173",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quantization Class\n",
    "import onnxruntime as ort\n",
    "from onnxruntime import quantization\n",
    "import timm\n",
    "\n",
    "class OnnxStaticQuantization:\n",
    "    def __init__(self) -> None:\n",
    "        self.enum_data = None\n",
    "        self.calibration_technique = {\n",
    "            \"MinMax\": ort.quantization.calibrate.CalibrationMethod.MinMax,\n",
    "            \"Entropy\": ort.quantization.calibrate.CalibrationMethod.Entropy,\n",
    "            \"Percentile\": ort.quantization.calibrate.CalibrationMethod.Percentile,\n",
    "            \"Distribution\": ort.quantization.calibrate.CalibrationMethod.Distribution\n",
    "        }\n",
    "    def get_next(self, EP_list = ['CPUExecutionProvider']):\n",
    "        if self.enum_data is None:\n",
    "            session = ort.InferenceSession(self.fp32_onnx_path, providers=EP_list)\n",
    "            input_name = session.get_inputs()[0].name\n",
    "            calib_list = []\n",
    "            count = 0\n",
    "            for nhwc_data, _ in self.calibration_loader:\n",
    "                nhwc_data=nhwc_data.cpu()\n",
    "                calib_list.append({input_name: nhwc_data.numpy()})\n",
    "                if self.sample == count: break\n",
    "                count = count + 1\n",
    "            self.enum_data = iter(calib_list)\n",
    "        return next(self.enum_data, None)\n",
    "    def quantization(self, fp32_onnx_path, future_int8_onnx_path, calib_method, calibration_loader, sample=100):\n",
    "        self.sample = sample\n",
    "        self.calibration_loader = calibration_loader\n",
    "        _ = ort.quantization.quantize_static(\n",
    "                model_input=fp32_onnx_path,\n",
    "                model_output=future_int8_onnx_path,\n",
    "                activation_type=ort.quantization.QuantType.QInt16, weight_type=ort.quantization.QuantType.QInt8,\n",
    "                calibrate_method=self.calibration_technique[calib_method],\n",
    "                calibration_data_reader=self\n",
    "            )\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988eddc6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# Perform the quantization\n",
    "val_dataset = timm.data.ImageDataset('/kaggle/working/datasets/coco/images/val2017')\n",
    "val_loader = timm.data.create_loader(val_dataset, (1,3,640,640), 1)\n",
    "module = OnnxStaticQuantization()\n",
    "module.fp32_onnx_path = \"/kaggle/working/yolov5/yolov5n_fp32.onnx\"\n",
    "\n",
    "# method=Entropy, calibration_number=1000\n",
    "module.quantization(\n",
    "    fp32_onnx_path=\"/kaggle/working/yolov5/yolov5n_fp32.onnx\",\n",
    "    future_int8_onnx_path=\"/kaggle/working/yolov5/yolov5n_fp32_Entropy.onnx\",\n",
    "    calib_method=\"Entropy\",\n",
    "    calibration_loader=val_loader,\n",
    "    sample=100   # calibration number\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0808f9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validation yolov5n_fp32_int8.onnx on COCO val\n",
    "!python val.py --weights /kaggle/working/yolov5/yolov5n_fp32_Entropy.onnx --data coco.yaml --img 640 --device 0 --task val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704001fe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quantization Class\n",
    "import onnxruntime as ort\n",
    "from onnxruntime import quantization\n",
    "import timm\n",
    "\n",
    "class OnnxStaticQuantization:\n",
    "    def __init__(self) -> None:\n",
    "        self.enum_data = None\n",
    "        self.calibration_technique = {\n",
    "            \"MinMax\": ort.quantization.calibrate.CalibrationMethod.MinMax,\n",
    "            \"Entropy\": ort.quantization.calibrate.CalibrationMethod.Entropy,\n",
    "            \"Percentile\": ort.quantization.calibrate.CalibrationMethod.Percentile,\n",
    "            \"Distribution\": ort.quantization.calibrate.CalibrationMethod.Distribution\n",
    "        }\n",
    "    def get_next(self, EP_list = ['CPUExecutionProvider']):\n",
    "        if self.enum_data is None:\n",
    "            session = ort.InferenceSession(self.fp32_onnx_path, providers=EP_list)\n",
    "            input_name = session.get_inputs()[0].name\n",
    "            calib_list = []\n",
    "            count = 0\n",
    "            for nhwc_data, _ in self.calibration_loader:\n",
    "                nhwc_data=nhwc_data.cpu()\n",
    "                calib_list.append({input_name: nhwc_data.numpy()})\n",
    "                if self.sample == count: break\n",
    "                count = count + 1\n",
    "            self.enum_data = iter(calib_list)\n",
    "        return next(self.enum_data, None)\n",
    "    def quantization(self, fp32_onnx_path, future_int8_onnx_path, calib_method, calibration_loader, sample=100):\n",
    "        self.sample = sample\n",
    "        self.calibration_loader = calibration_loader\n",
    "        _ = ort.quantization.quantize_static(\n",
    "                model_input=fp32_onnx_path,\n",
    "                model_output=future_int8_onnx_path,\n",
    "                activation_type=ort.quantization.QuantType.QInt16, weight_type=ort.quantization.QuantType.QInt8,\n",
    "                calibrate_method=self.calibration_technique[calib_method],\n",
    "                calibration_data_reader=self\n",
    "            )\n",
    "        return self\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb8f964",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "   \n",
    "# ====================================================================================\n",
    "# Perform the quantization\n",
    "val_dataset = timm.data.ImageDataset('/kaggle/working/datasets/coco/images/val2017')\n",
    "val_loader = timm.data.create_loader(val_dataset, (1,3,640,640), 1)\n",
    "module = OnnxStaticQuantization()\n",
    "module.fp32_onnx_path = \"/kaggle/working/yolov5/yolov5n_fp32.onnx\"\n",
    "\n",
    "# method=Percentile, calibration_number=1000\n",
    "module.quantization(\n",
    "    fp32_onnx_path=\"/kaggle/working/yolov5/yolov5n_fp32.onnx\",\n",
    "    future_int8_onnx_path=\"/kaggle/working/yolov5/yolov5n_fp32_Percentile.onnx\",\n",
    "    calib_method=\"Percentile\",\n",
    "    calibration_loader=val_loader,\n",
    "    sample=500   # calibration number\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74193f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validation yolov5n_fp32_Percentile.onnx on COCO val\n",
    "!python val.py --weights /kaggle/working/yolov5/yolov5n_fp32_Percentile.onnx --data coco.yaml --img 640 --device 0 --task val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98c0ef",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quantization Class\n",
    "import onnxruntime as ort\n",
    "from onnxruntime import quantization\n",
    "import timm\n",
    "\n",
    "class OnnxStaticQuantization:\n",
    "    def __init__(self) -> None:\n",
    "        self.enum_data = None\n",
    "        self.calibration_technique = {\n",
    "            \"MinMax\": ort.quantization.calibrate.CalibrationMethod.MinMax,\n",
    "            \"Entropy\": ort.quantization.calibrate.CalibrationMethod.Entropy,\n",
    "            \"Percentile\": ort.quantization.calibrate.CalibrationMethod.Percentile,\n",
    "            \"Distribution\": ort.quantization.calibrate.CalibrationMethod.Distribution\n",
    "        }\n",
    "    def get_next(self, EP_list = ['CPUExecutionProvider']):\n",
    "        if self.enum_data is None:\n",
    "            session = ort.InferenceSession(self.fp32_onnx_path, providers=EP_list)\n",
    "            input_name = session.get_inputs()[0].name\n",
    "            calib_list = []\n",
    "            count = 0\n",
    "            for nhwc_data, _ in self.calibration_loader:\n",
    "                nhwc_data=nhwc_data.cpu()\n",
    "                calib_list.append({input_name: nhwc_data.numpy()})\n",
    "                if self.sample == count: break\n",
    "                count = count + 1\n",
    "            self.enum_data = iter(calib_list)\n",
    "        return next(self.enum_data, None)\n",
    "    def quantization(self, fp32_onnx_path, future_int8_onnx_path, calib_method, calibration_loader, sample=100):\n",
    "        self.sample = sample\n",
    "        self.calibration_loader = calibration_loader\n",
    "        _ = ort.quantization.quantize_static(\n",
    "                model_input=fp32_onnx_path,\n",
    "                model_output=future_int8_onnx_path,\n",
    "                activation_type=ort.quantization.QuantType.QInt16, weight_type=ort.quantization.QuantType.QInt8,\n",
    "                calibrate_method=self.calibration_technique[calib_method],\n",
    "                calibration_data_reader=self\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    \n",
    "# ====================================================================================\n",
    "# Perform the quantization\n",
    "val_dataset = timm.data.ImageDataset('/kaggle/working/datasets/coco/images/val2017')\n",
    "val_loader = timm.data.create_loader(val_dataset, (1,3,640,640), 1)\n",
    "module = OnnxStaticQuantization()\n",
    "module.fp32_onnx_path = \"/kaggle/working/yolov5/yolov5n_fp32.onnx\"\n",
    "\n",
    "# method=Distribution, calibration_number=5000\n",
    "# Perform the quantization\n",
    "val_dataset = timm.data.ImageDataset('/kaggle/working/datasets/coco/images/val2017')\n",
    "val_loader = timm.data.create_loader(val_dataset, (1,3,640,640), 1)\n",
    "module = OnnxStaticQuantization()\n",
    "module.fp32_onnx_path = \"/kaggle/working/yolov5/yolov5n_fp32.onnx\"\n",
    "\n",
    "module.quantization(\n",
    "    fp32_onnx_path=\"/kaggle/working/yolov5/yolov5n_fp32.onnx\",\n",
    "    future_int8_onnx_path=\"/kaggle/working/yolov5/yolov5n_fp32_Distribution.onnx\",\n",
    "    calib_method=\"Distribution\",\n",
    "    calibration_loader=val_loader,\n",
    "    sample=1000   # calibration number\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb31222",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validation yolov5n_fp32_int8.onnx on COCO val\n",
    "!python val.py --weights /kaggle/working/yolov5/yolov5n_fp32_Distribution.onnx --data coco.yaml --img 640 --device 0 --task val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5561f1d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## **TVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa65e2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823260ea",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TVM : Tensor Virtual Machine\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import onnx\n",
    "import tvm\n",
    "import tvm.relay as relay\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Import the data_folder and transform variables\n",
    "data_folder = '/path/to/your/data/folder'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Test Dataset\n",
    "test_dataset = datasets.ImageFolder(root=data_folder, transform=transform)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "subset_indices = list(range(0, len(test_dataset), 50))\n",
    "subset_dataset = Subset(test_dataset, subset_indices)\n",
    "test_loader = DataLoader(subset_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Tensor Virtual Machine\n",
    "def eval_tvm(args, data_loader):\n",
    "    input_shape = (1, 3, 640, 640)\n",
    "    model_path = args.model + \"_int8\" + \".onnx\"\n",
    "    onnx_model = onnx.load(model_path)\n",
    "    input_name = \"input\"\n",
    "    shape_dict = {input_name: input_shape}\n",
    "    mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "    target = \"llvm\"\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        executor = relay.build_module.create_executor(\n",
    "            \"graph\", mod, tvm.cpu(0), target, params\n",
    "        ).evaluate()\n",
    "\n",
    "    print(\"Finished TVM conversion\")\n",
    "\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total_samples = 0\n",
    "    start_time = time.time()\n",
    "    for idx, (images, labels) in enumerate(data_loader):\n",
    "        # Set the input data\n",
    "        numpy_images = images.numpy()\n",
    "        input_data = tvm.nd.array(numpy_images.astype(\"float32\"))\n",
    "        tvm_output = executor(input_data).numpy()\n",
    "        predicted_labels = np.argmax(tvm_output, axis=1)\n",
    "        top1_correct += np.sum(predicted_labels == labels.numpy())\n",
    "\n",
    "        # Calculate top-5 accuracy\n",
    "        top5_predicted_labels = np.argsort(tvm_output, axis=1)[:, -5:]\n",
    "        for i in range(labels.size(0)):\n",
    "            if labels.numpy()[i] in top5_predicted_labels[i]:\n",
    "                top5_correct += 1\n",
    "\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        if idx >= 999:  # Process 1000 batches\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    print(\"Inference Time:\", inference_time, \"seconds\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    top1_accuracy = top1_correct / total_samples\n",
    "    top5_accuracy = top5_correct / total_samples\n",
    "\n",
    "    print(f\"Top-1 Accuracy: {top1_accuracy * 100:.2f}%\")\n",
    "    print(f\"Top-5 Accuracy: {top5_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "args = {'model': '/path/to/your/model'}\n",
    "eval_tvm(args, test_loader)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 996.865517,
   "end_time": "2024-08-27T04:36:25.537075",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-27T04:19:48.671558",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
