{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebcdc04f",
   "metadata": {
    "papermill": {
     "duration": 0.005727,
     "end_time": "2024-04-16T12:25:26.700914",
     "exception": false,
     "start_time": "2024-04-16T12:25:26.695187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Transformers**\n",
    "- Data does not always come in its final processed form that is required for training machine learning algorithms. We use transforms to perform some manipulation of the data and make it suitable for training.\n",
    "- All TorchVision datasets have two parameters :\n",
    "    - `transform` to modify the **features** and `target_transform` to modify the **labels** - that accept callables containing the transformation logic. \n",
    "    - The ***torchvision.transforms*** module offers several commonly-used transforms out of the box.\n",
    "    - The _FashionMNIST_ features are in PIL Image format, and the labels are integers. For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors. To make these transformations, we use `ToTensor` and `Lambda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a355bd94",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-16T12:25:26.714207Z",
     "iopub.status.busy": "2024-04-16T12:25:26.713143Z",
     "iopub.status.idle": "2024-04-16T12:25:57.808176Z",
     "shell.execute_reply": "2024-04-16T12:25:57.806558Z"
    },
    "papermill": {
     "duration": 31.105332,
     "end_time": "2024-04-16T12:25:57.811527",
     "exception": false,
     "start_time": "2024-04-16T12:25:26.706195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2+cpu)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2+cpu)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\r\n",
      "Requirement already satisfied: torch==2.1.2 in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.1.2+cpu)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.2->torchvision) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.2->torchvision) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc5c23ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T12:25:57.825267Z",
     "iopub.status.busy": "2024-04-16T12:25:57.824705Z",
     "iopub.status.idle": "2024-04-16T12:26:15.221012Z",
     "shell.execute_reply": "2024-04-16T12:26:15.219388Z"
    },
    "papermill": {
     "duration": 17.408013,
     "end_time": "2024-04-16T12:26:15.225237",
     "exception": false,
     "start_time": "2024-04-16T12:25:57.817224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:03<00:00, 6837305.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 140054.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:01<00:00, 2568286.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 6497826.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),  # normalized tensor\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)) # one hot encoding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0239397c",
   "metadata": {
    "papermill": {
     "duration": 0.010485,
     "end_time": "2024-04-16T12:26:15.246613",
     "exception": false,
     "start_time": "2024-04-16T12:26:15.236128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **ToTensor()**\n",
    "- `ToTensor()` converts a PIL image or NumPy `ndarray` into a `FloatTensor`, and scales the image's pixel intensity values in the range [0., 1.].\n",
    "## **Lambda Transforms**\n",
    "- Lambda transforms apply any user-defined lambda function. Here, we define a function to turn the integer into a one-hot encoded tensor. It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls **scatter_** which assigns a `value=1` on the index as given by the label `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "102db462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T12:26:15.269192Z",
     "iopub.status.busy": "2024-04-16T12:26:15.268623Z",
     "iopub.status.idle": "2024-04-16T12:26:15.274334Z",
     "shell.execute_reply": "2024-04-16T12:26:15.273212Z"
    },
    "papermill": {
     "duration": 0.020444,
     "end_time": "2024-04-16T12:26:15.277142",
     "exception": false,
     "start_time": "2024-04-16T12:26:15.256698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d470f5",
   "metadata": {
    "papermill": {
     "duration": 0.009969,
     "end_time": "2024-04-16T12:26:15.297992",
     "exception": false,
     "start_time": "2024-04-16T12:26:15.288023",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Transforming and Augmenting Images**\n",
    "- Torchvision supports common computer vision transformations in the `torchvision.transforms` and `torchvision.transforms.v2` modules. Transforms can be used to transform or augment data for training or inference of different tasks (image classification, detection, segmentation, video classification).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c8dd40b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T12:26:15.320367Z",
     "iopub.status.busy": "2024-04-16T12:26:15.319917Z",
     "iopub.status.idle": "2024-04-16T12:26:15.481557Z",
     "shell.execute_reply": "2024-04-16T12:26:15.480300Z"
    },
    "papermill": {
     "duration": 0.176034,
     "end_time": "2024-04-16T12:26:15.484104",
     "exception": false,
     "start_time": "2024-04-16T12:26:15.308070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0116, -0.0116, -0.0116,  ...,  2.1804,  2.1804,  2.1804],\n",
       "         [-0.0116, -0.0116, -0.0116,  ...,  2.1804,  2.1804,  2.1804],\n",
       "         [-0.0116, -0.0116, -0.0116,  ...,  2.1804,  2.1804,  2.1804],\n",
       "         ...,\n",
       "         [ 0.8618,  0.8618,  0.8618,  ..., -0.6965, -0.6965, -0.6965],\n",
       "         [ 0.8618,  0.8618,  0.8618,  ..., -0.6965, -0.6965, -0.6965],\n",
       "         [ 0.8618,  0.8618,  0.8618,  ..., -0.6965, -0.6965, -0.6965]],\n",
       "\n",
       "        [[ 1.7108,  1.7108,  1.7108,  ...,  2.2360,  2.2360,  2.2360],\n",
       "         [ 1.7108,  1.7108,  1.7108,  ...,  2.2360,  2.2360,  2.2360],\n",
       "         [ 1.7108,  1.7108,  1.7108,  ...,  2.2360,  2.2360,  2.2360],\n",
       "         ...,\n",
       "         [ 2.2885,  2.2885,  2.2885,  ..., -0.1800, -0.1800, -0.1800],\n",
       "         [ 2.2885,  2.2885,  2.2885,  ..., -0.1800, -0.1800, -0.1800],\n",
       "         [ 2.2885,  2.2885,  2.2885,  ..., -0.1800, -0.1800, -0.1800]],\n",
       "\n",
       "        [[ 0.3219,  0.3219,  0.3219,  ..., -1.2467, -1.2467, -1.2467],\n",
       "         [ 0.3219,  0.3219,  0.3219,  ..., -1.2467, -1.2467, -1.2467],\n",
       "         [ 0.3219,  0.3219,  0.3219,  ..., -1.2467, -1.2467, -1.2467],\n",
       "         ...,\n",
       "         [-0.1312, -0.1312, -0.1312,  ...,  1.2980,  1.2980,  1.2980],\n",
       "         [-0.1312, -0.1312, -0.1312,  ...,  1.2980,  1.2980,  1.2980],\n",
       "         [-0.1312, -0.1312, -0.1312,  ...,  1.2980,  1.2980,  1.2980]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Image Classification\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "H, W = 32, 32\n",
    "img = torch.randint(0, 256, size=(3, H, W), dtype=torch.uint8)\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "img = transforms(img)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6cc5e9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T12:26:15.507453Z",
     "iopub.status.busy": "2024-04-16T12:26:15.506108Z",
     "iopub.status.idle": "2024-04-16T12:26:15.537628Z",
     "shell.execute_reply": "2024-04-16T12:26:15.536267Z"
    },
    "papermill": {
     "duration": 0.046175,
     "end_time": "2024-04-16T12:26:15.540549",
     "exception": false,
     "start_time": "2024-04-16T12:26:15.494374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[ 2.3932,  2.3932,  2.4937,  ...,  1.5820,  1.4959,  1.4959],\n",
       "          [ 2.3932,  2.3932,  2.4937,  ...,  1.5820,  1.4959,  1.4959],\n",
       "          [ 2.4118,  2.4118,  2.5087,  ...,  1.7523,  1.6626,  1.6626],\n",
       "          ...,\n",
       "          [-0.2803, -0.2803, -0.2302,  ..., -2.4676, -2.4496, -2.4496],\n",
       "          [-0.2989, -0.2989, -0.2558,  ..., -2.5566, -2.5423, -2.5423],\n",
       "          [-0.2989, -0.2989, -0.2558,  ..., -2.5566, -2.5423, -2.5423]],\n",
       " \n",
       "         [[-1.8231, -1.8231, -1.8531,  ...,  1.6670,  1.7721,  1.7721],\n",
       "          [-1.8231, -1.8231, -1.8531,  ...,  1.6670,  1.7721,  1.7721],\n",
       "          [-1.7650, -1.7650, -1.7950,  ...,  1.8376,  1.9463,  1.9463],\n",
       "          ...,\n",
       "          [ 5.2504,  5.2504,  5.2429,  ...,  0.6986,  0.6986,  0.6986],\n",
       "          [ 5.2891,  5.2891,  5.2891,  ...,  0.8342,  0.8342,  0.8342],\n",
       "          [ 5.2891,  5.2891,  5.2891,  ...,  0.8342,  0.8342,  0.8342]],\n",
       " \n",
       "         [[-1.1483, -1.1483, -1.1186,  ..., -4.3008, -4.4792, -4.4792],\n",
       "          [-1.1483, -1.1483, -1.1186,  ..., -4.3008, -4.4792, -4.4792],\n",
       "          [-1.3786, -1.3786, -1.3489,  ..., -4.2012, -4.3833, -4.3833],\n",
       "          ...,\n",
       "          [-5.4273, -5.4273, -5.4719,  ...,  7.6465,  7.7208,  7.7208],\n",
       "          [-5.3313, -5.3313, -5.3759,  ...,  7.6081,  7.6825,  7.6825],\n",
       "          [-5.3313, -5.3313, -5.3759,  ...,  7.6081,  7.6825,  7.6825]]]),\n",
       " 'boxes': BoundingBoxes([[  0,   0,   0, 103],\n",
       "                [  0,   0, 127,   0],\n",
       "                [  0,   0,   0,   0]], format=BoundingBoxFormat.XYXY, canvas_size=(224, 224))}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detection (re-using imports and transforms from above)\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "img = torch.randint(0, 256, size=(3, H, W), dtype=torch.uint8)\n",
    "boxes = torch.randint(0, H // 2, size=(3, 4))\n",
    "boxes[:, 2:] += boxes[:, :2]\n",
    "boxes = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(H, W))\n",
    "\n",
    "# The same transforms can be used!\n",
    "img, boxes = transforms(img, boxes)\n",
    "# And you can pass arbitrary input structures\n",
    "output_dict = transforms({\"image\": img, \"boxes\": boxes})\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f6b53",
   "metadata": {
    "papermill": {
     "duration": 0.010139,
     "end_time": "2024-04-16T12:26:15.561247",
     "exception": false,
     "start_time": "2024-04-16T12:26:15.551108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Supported input types and conventions\n",
    "- Tensor image are expected to be of shape (`C`, `H`, `W`), where `C` is the number of channels, and `H` and `W` refer to height and width. Most transforms support batched tensor input. A batch of Tensor images is a tensor of shape (`N`, `C`, `H`, `W`), where `N` is a number of images in the batch. The ***v2*** transforms generally accept an arbitrary number of leading dimensions (`...`, `C`, `H`, `W`) and can handle batched images or batched videos.\n",
    "\n",
    "### Dtype and expected value range \n",
    "- The expected range of the values of a tensor image is implicitly defined by the tensor dtype. Tensor images with a float dtype are expected to have values in `[0, 1]`. Tensor images with an integer dtype are expected to have values in `[0, MAX_DTYPE]` where `MAX_DTYPE` is the largest value that can be represented in that dtype. Typically, images of dtype `torch.uint8` are expected to have values in `[0, 255]`.\n",
    "\n",
    "### V1 or V2? Which one should I use?\n",
    "- We recommending using the `torchvision.transforms.v2` transforms instead of those in `torchvision.transforms`. They’re faster and they can do more things. Just change the import and you should be good to go. Moving forward, new features and improvements will only be considered for the v2 transforms.\n",
    "- In Torchvision 0.15 (March 2023), we released a new set of transforms available in the `torchvision.transforms.v2` namespace. These transforms have a lot of advantages compared to the v1 ones (in `torchvision.transforms`):\n",
    "    -  They can transform ***images*** but also ***bounding boxes***, ***masks***, or ***videos***. This provides support for tasks beyond _image classification_: _detection_, _segmentation_, _video classification_, etc.\n",
    "    - They support more transforms like `CutMix` and `MixUp`.\n",
    "    - They support arbitary input structures(dicts, lists, tuples, etc)\n",
    "    - Future improvements and features will be added to the v2 transforms only.\n",
    "- These transforms are fully backward compatible with the v1 ones, so if you’re already using tranforms from `torchvision.transforms`, all you need to do to is to update the import to `torchvision.transforms.v2`. In terms of output, there might be negligible differences due to implementation differences.\n",
    "### Performance Considerations\n",
    "- We recommend the following guidelines to get the best performance out of the transforms:\n",
    "    - Rely on the v2 transforms from `torchvision.transforms.v2`\n",
    "    - Use tensors instead of PIL images.\n",
    "    - Use `torch.uint8` dtype, especially for resizing.\n",
    "    - Resize with bilinear or bicubic mode.\n",
    "- ***Note :*** Note that resize transforms like `Resize` and `RandomResizedCrop` typically prefer __channels-last__ input and tend not to benefit from `torch.compile()` at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0794acd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T12:26:15.585576Z",
     "iopub.status.busy": "2024-04-16T12:26:15.585155Z",
     "iopub.status.idle": "2024-04-16T12:26:15.596544Z",
     "shell.execute_reply": "2024-04-16T12:26:15.595246Z"
    },
    "papermill": {
     "duration": 0.027475,
     "end_time": "2024-04-16T12:26:15.599177",
     "exception": false,
     "start_time": "2024-04-16T12:26:15.571702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "      ToImage()\n",
       "      ToDtype(scale=True)\n",
       "      RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
       "      ToDtype(scale=True)\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.transforms import v2\n",
    "transforms = v2.Compose([\n",
    "    v2.ToImage(),  # Convert to tensor, only needed if you had a PIL image\n",
    "    v2.ToDtype(torch.uint8, scale=True),  # optional, most input are already uint8 at this point\n",
    "    # ...\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),  # Or Resize(antialias=True)\n",
    "    # ...\n",
    "    v2.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61706dc3",
   "metadata": {
    "papermill": {
     "duration": 0.010163,
     "end_time": "2024-04-16T12:26:15.620138",
     "exception": false,
     "start_time": "2024-04-16T12:26:15.609975",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Transform classes, functionals, and kernels**\n",
    "- Transforms are available as classes like `Resize`, but also as functionals like `resize()` in the `torchvision.transforms.v2.functional` namespace. This is very much like the `torch.nn` package which defines both classes and functional equivalents in `torch.nn.functional`.\n",
    "- The functionals support PIL images, pure tensors, or **TVTensors**, e.g. both `resize(image_tensor)` and `resize(boxes)` are valid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c6864",
   "metadata": {
    "papermill": {
     "duration": 0.010327,
     "end_time": "2024-04-16T12:26:15.641879",
     "exception": false,
     "start_time": "2024-04-16T12:26:15.631552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 53.94144,
   "end_time": "2024-04-16T12:26:17.379838",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-16T12:25:23.438398",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
