{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc9987a8-40e2-4378-aec4-c5dfdc0cc067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bbf374-cc8c-4684-8e23-bda3ddb360eb",
   "metadata": {},
   "source": [
    "### **torch.nn.Linear()**\n",
    "- **Note** : A matrix multiplication like this is also referred to as the `dot product` of two matrices.\n",
    "- Nural network are full of matrix multiplications and dot products.\n",
    "- The `torch.nn.Linear()` module (we'll see this in action later on), also known as a __feed-forward layer__ of __fully connected layer__, implements a matrix multiplication between an input `x` and weights matrix `A`.\n",
    "- `y = x.transpose(A) + b`\n",
    "  - Where :\n",
    "  - `x` is the input to the layer (deep learning is a stack of layers like `torch.nn.Linear()` and others on top of each other.)\n",
    "  - `A` is the weights matrix created by the layer, this starts out as random numbers that get ajusted as a Neural Netwok learns to better representation patterns in the data. (notice the \"`T`\", that's because the weights matrix get transposed).\n",
    "  - __Note__ : You might also often see `W` or another letter like `X` used to showcase the weights matrix.\n",
    "  - `b` is the bias terms used to slightly offset the weights and inputs.\n",
    "  - `y` is the output (a manipulation of the input in the hopes to discover patterns in it).\n",
    "  - This the linear function linek `y = mx + c` equation of straight line. Try to changing the values of `in_features` and `out_features` below and see what happens.\n",
    "  - Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8535be7-fa84-4c91-b885-eb1cff03a844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape : torch.Size([3, 2])\n",
      "\n",
      "Output : \n",
      "tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n",
      "        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n",
      "        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Ouptu Shape : torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\n",
    "torch.manual_seed(42)\n",
    "linear = torch.nn.Linear(in_features=2,   # matche inner dimension of input\n",
    "                         out_features=6)  # describe outer value\n",
    "\n",
    "x = torch.tensor([[1, 2],   # put 2 feature in a row of input\n",
    "                  [3, 4],\n",
    "                  [5, 6]], dtype=torch.float)\n",
    "\n",
    "output = linear(x)   # get 6 feature in a row of output \n",
    "print(f\"Input Shape : {x.shape}\\n\")\n",
    "print(f\"Output : \\n{output}\\n\\nOuptu Shape : {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a99737b3-8767-4d14-8c90-5ea7164a7a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 2.,  4.,  6.,  8.],\n",
      "        [ 3.,  6.,  9., 12.],\n",
      "        [ 4.,  8., 12., 16.],\n",
      "        [ 5., 10., 15., 20.],\n",
      "        [ 6., 12., 18., 24.]])\n",
      "torch.Size([6, 4])\n",
      "\n",
      "tensor([[  0.4974,  -1.5763,   1.4562,  -0.8506,   0.7215,  -0.4398,  -3.2376,\n",
      "          -0.5411,   1.0880,   0.6005],\n",
      "        [  1.3022,  -3.2688,   2.6275,  -1.3393,   1.8976,  -1.1590,  -5.9811,\n",
      "          -0.7089,   1.9592,   1.3687],\n",
      "        [  2.1071,  -4.9613,   3.7988,  -1.8280,   3.0737,  -1.8782,  -8.7246,\n",
      "          -0.8768,   2.8304,   2.1368],\n",
      "        [  2.9119,  -6.6538,   4.9701,  -2.3167,   4.2497,  -2.5974, -11.4682,\n",
      "          -1.0447,   3.7016,   2.9050],\n",
      "        [  3.7168,  -8.3462,   6.1413,  -2.8053,   5.4258,  -3.3166, -14.2117,\n",
      "          -1.2125,   4.5729,   3.6732],\n",
      "        [  4.5216, -10.0387,   7.3126,  -3.2940,   6.6019,  -4.0358, -16.9552,\n",
      "          -1.3804,   5.4441,   4.4413]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([6, 10])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(32)\n",
    "linearModel = torch.nn.Linear(in_features=4, # matche inner dimension of input\n",
    "                         out_features=10) # describe outer value\n",
    "  \n",
    "x = torch.tensor([[1, 2, 3, 4],           # put 4 feature in a row of input\n",
    "                  [2, 4, 6, 8],\n",
    "                  [3, 6, 9, 12],\n",
    "                  [4, 8, 12, 16],\n",
    "                  [5, 10, 15, 20],\n",
    "                  [6, 12, 18, 24]], dtype=torch.float)\n",
    "\n",
    "y = linearModel(x)                         # get 10 feature in a row of output\n",
    "print(x); print(x.shape); print()\n",
    "print(y); print(y.shape); print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5048ea-9893-4bb8-a8df-6c6a2b7a535f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55d87e37-410d-476e-82c0-64fe5ab28b1f",
   "metadata": {},
   "source": [
    "## **<font color=\"red\">Reproducibility : (Trying to take the random out of random)</font>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf730370-3dc2-4c2a-a992-d24f73b90aab",
   "metadata": {},
   "source": [
    "- As you learn more about Neural Netwoks and Machine Learning, you'll start to discover how much randomness plays a part.\n",
    "- How does this related to nural networks and deep learning then?\n",
    "- We've discussed neural networks start with random numbers to describe patterns in data and try to improve those random numbers using tensor operations to better describe patterns in data.\n",
    "- In short : `start with random numbers` ---> `tensor operations` ---> `try to make better (again and again......)`\n",
    "- Although randomness is nice and powerful, sometimes you'd like there to be a little less randomness. Because, for example, I create an algorithm capable of achieving `X` performance and now I want to verify that I'm right or wrong.\n",
    "- To verify that I am right and my work is in correct direction the `Reproducibility` comes in picture.\n",
    "- Any one get the same results on his computer running the same code as I get on mine.\n",
    "- Let's see a brief example of reproducibility in PyTorch.\n",
    "- We'll start by creating two random tensors, since they're random, you'd expect them to be different right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc53507-a14f-4b5f-9582-37defa7ba471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor A : \n",
      "tensor([[0.8529, 0.3920, 0.5805, 0.2238],\n",
      "        [0.5989, 0.0382, 0.1198, 0.1159],\n",
      "        [0.5589, 0.4112, 0.9977, 0.5187]])\n",
      "\n",
      "tensor B : \n",
      "tensor([[0.9310, 0.2028, 0.6240, 0.4142],\n",
      "        [0.9531, 0.6668, 0.5360, 0.1804],\n",
      "        [0.0755, 0.3405, 0.7576, 0.2296]])\n",
      "\n",
      "Does Tensor A equal Tensor B? (anywhere)\n",
      "tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "random_tensor_a = torch.rand(size=(3, 4))\n",
    "random_tensor_b = torch.rand(size=(3, 4))\n",
    "print(f\"tensor A : \\n{random_tensor_a}\\n\")\n",
    "print(f\"tensor B : \\n{random_tensor_b}\\n\")\n",
    "print(f\"Does Tensor A equal Tensor B? (anywhere)\")\n",
    "print(random_tensor_a == random_tensor_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa58222-04a1-42f1-9955-98e6e39a6e32",
   "metadata": {},
   "source": [
    "- Just as you might've expected, the tensors come out with different values. But what if you wnated to created two random tensors with the same values.\n",
    "- As in, the tensors would still contain random values but they would be of the same flavour. That's where `torch.manual_seed(seed)` comes in, where `seed` is an integer that flavours the randomness.\n",
    "- Let's try it out by creating some more *flavoured* random tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4a8998-d1f4-4cce-89b9-331f0b2024ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor C : \n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Tensor D : \n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Does Tensor C equal Tensor D? (anywhere)\n",
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# torch.random.manual_seed(seed=RANDOM_SEED) # reset the seed every time a new rand()\n",
    "random_tensor_C = torch.rand(3, 4)\n",
    "\n",
    "# Reset the seed every time a new rand().\n",
    "# Without this, tensor_D would be different to tensor_C\n",
    "torch.random.manual_seed(seed=RANDOM_SEED)  # without this line get different tensor\n",
    "random_tensor_D = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Tensor C : \\n{random_tensor_C}\\n\")\n",
    "print(f\"Tensor D : \\n{random_tensor_D}\\n\")\n",
    "print(f\"Does Tensor C equal Tensor D? (anywhere)\")\n",
    "print(random_tensor_C == random_tensor_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af64d7f-d595-431d-bd90-830ec03c4d91",
   "metadata": {},
   "source": [
    "- Completely reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds.\n",
    "- However, there are some steps you can take to limit the number of sources of nondeterministic behavior for a specific platform, device, and PyTorch release. **First**, you can control sources of randomness that can cause multiple executions of your application to behave differently. **Second**, you can configure PyTorch to avoid using nondeterministic algorithms for some operations, so that multiple calls to those operations, given the same inputs, will produce the same result.\n",
    "- **Controlling Sources of Randomness** :\n",
    "  - **PyTorch Random Number Generator** :\n",
    "  - You can use `torch.manual_seed() to seed the RNG for all devices (both CPU and CUDA).\n",
    "  - Some PyTorch operations may use random numbers internally. `torch.svd_lowrank()` does this, for instance. Consequently, calling it multiple times back-to-back with the same input arguments may give different results. However, as long as `torch.manual_seed()` is set to a constant at the beginning of an application and all other sources of nondeterminism have been eliminated, the same series of random numbers will be generated each time the application is run in the same environment.\n",
    "  - It is also possible to obtain identical results from an operation that uses random numbers by setting `torch.manual_seed()` to the same value between subsequent calls.\n",
    "  - **PyTorch**:\n",
    "    <pre>\n",
    "        import torch\n",
    "        torch.manual_seed(0)\n",
    "    </pre>\n",
    "  - **Python** :\n",
    "    <pre>\n",
    "        import random\n",
    "        random.seed(0)\n",
    "    </pre>\n",
    "  - **NumPy**:\n",
    "    <pre>\n",
    "        import numpy as np\n",
    "        np.random.seed(0)\n",
    "    </pre>\n",
    "### **CUDA Convolution Benchmarking**\n",
    "- The `cuDNN` library, used by **CUDA** convolution operations, can be a source of nondeterminism across multiple executions of an application. When a `cuDNN` convolution is called with a new set of size parameters, an optional feature can run multiple convolution algorithms, benchmarking them to find the fastest one. Then, the fastest algorithm will be used consistently during the rest of the process for the corresponding set of size parameters. Due to benchmarking noise and different hardware, the benchmark may select different algorithms on subsequent runs, even on the same machine.\n",
    "- Disabling the benchmarking feature with `torch.backends.cudnn.benchmark = False` causes cuDNN to deterministically select an algorithm, possibly at the cost of reduced performance.\n",
    "- However, if you do not need reproducibility across multiple executions of your application, then performance might improve if the benchmarking feature is enabled with `torch.backends.cudnn.benchmark = True`.\n",
    "- **Note**: This setting is different from the `torch.backends.cudnn.deterministic` setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336546b4-aaad-4557-9d19-af83be5873bf",
   "metadata": {},
   "source": [
    "## **<font color=\"red\">Running tensors on GPUs (and making faster computations)</font>**\n",
    "- Deep learning algorithms require a lot of numerical operations. And by default these operations are often done on a `CPU` (Computer Processing Unit).\n",
    "- However, there's another common piece of hardware called a `GPU` (Graphic Processing Unit), which is often much faster at performing the specific type of operations neural networks need (matrix multiplications) than CPUs.\n",
    "- Your computer might have one. If so, you should look to use it whenever you can to train neural networks because chances are it'll speed up the training time dramatically.\n",
    "- There are few ways to first get access to a GPU and secondly get PyTorch to use the GPU.\n",
    "- **Note** : When I reference `GPU` throughout this course, I'm referencing a `Nvidia GPU with CUDA` enabled (CUDA is a computing platform and API that helps allow GPUs be used for general purpose computing & not just graphics) unless otherwise specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ca8de-24a2-4164-85a1-9de415be1bf6",
   "metadata": {},
   "source": [
    "### **1. Getting a GPU**\n",
    "- You may already know what's going on when I say GPU. But if not, there are a few ways to get access to one.\n",
    "- 1. Method : ***Google Colab*** : Free to use, almost zero setup required, can share work with others as easy as link.\n",
    "  2. Method : ***Use you own*** : Run everything locally on your own machine.\n",
    "  3. Method : ***Cloud Computing (AWS, GCP, Azure)*** : Small upfront cost, access to almost infinite compute.\n",
    "  4. Time(s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
    "     - *CPU (s)* : 3.8624\n",
    "     - *GPU (s)* : 0.1083\n",
    "     - *GPU* speedup over *CPU* : 35x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eade02-938d-432d-a659-bfe0412fa27f",
   "metadata": {},
   "source": [
    "- For more knowloege about GPU you can follow this link:\n",
    "https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/\n",
    "- To check if you've got access to a Nvidia GPU, you can run `!nvidia-smi` where the `!` (also called bang) means \"run this on the command line\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "219e212b-1b25-42ec-9db0-f7b0afa4733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 25 12:36:18 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 532.09                 Driver Version: 532.09       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce MX330          WDDM | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   54C    P0               N/A /  N/A|      0MiB /  2048MiB |      2%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eb48c0-5388-49dc-8b75-997f2a7b2a2f",
   "metadata": {},
   "source": [
    "- If you don't have a Nvidia GPU accessible, the above will output something like:\n",
    "  - NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
    "  - In that case, go back up and follow the install steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e176c-918f-45f3-8630-c11f8f26c783",
   "metadata": {},
   "source": [
    "### **2. Getting PyTorch to run on the GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa69cfa5-6301-4c66-a2a2-e004695d1f0e",
   "metadata": {},
   "source": [
    "- Once you've got a GPU ready to access, the next step is getting PyTorch to use for storing data(tensors) and computing on data (performing operations on tensors).\n",
    "- To do so, you can use the `torch.cuda` package.\n",
    "- You can test if PyTorch has access to a GPU using `torch.cuda.is_available()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e238aae4-7e0e-4c9f-b512-5f2d26c8bf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f515a-f446-4431-a8ad-2a27c7074819",
   "metadata": {},
   "source": [
    "- If the above outputs `True`, PyTorch can see and use the GPU, if it outputs `False`, it can't see the GPU and in that case, you'll have to go back through the installation steps.\n",
    "- Now, let's say you wanted to setup your code so it ran on CPU or the GPU if it was available. That way, if you or someone decides to run your code, it'll work regardless of the computing device they're using.\n",
    "- Let's create a `device` variable to store what kind of device is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0e7d69a-674f-4aac-ad10-e5f1e893a710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device type\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device : \", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98bef89-5f9a-4a74-90cd-55abca5d219c",
   "metadata": {},
   "source": [
    "- If the above output `\"cuda\"` it means we can set all of our PyTorch code to use the available CUDA device (a GPU) and if it output `\"cpu\"`, our PyTorch code will stick with the CPU.\n",
    "  - **Note**: In PyTorch, it's best practice to write ***device agnostic code***. This means code that'll run on CPU (always available) or GPU (if available).\n",
    "- If you want to do faster computing you can use a GPU but if you want to do *much* faster computing, you can use multiple GPUs.\n",
    "- You can count the number of GPUs PyTorch has access to using `torch.cuda.device_count()`.\n",
    "- You can count the number of CPUs PyTorch has access to using `torch.cpu.device_count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f12b6500-73a2-48e5-adc0-e5fb87878b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda :  0\n",
      "cpu :  1\n"
     ]
    }
   ],
   "source": [
    "# Count number of devices\n",
    "print(\"cuda : \", torch.cuda.device_count())\n",
    "print(\"cpu : \", torch.cpu.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7fcb6d-2873-4f75-9260-e470c0bb502a",
   "metadata": {},
   "source": [
    "### **3. Putting tensors (and models) on the GPU**\n",
    "- `CPU` >---`tensor.to(device)`--->>> `GPU`\n",
    "- You can put tensors (and models, we'll see this later) on a specific device by calling `to(device)` on them. Where `device` is the target device you'd like the tensor (or model) to go to.\n",
    "- Why do this?\n",
    "  - GPUs offer for faster numerical computing than CPUs do and if a GPU isn't available, because of our ***device agnostic code***, it'll run on the CPU.\n",
    "- **Note :** Putting a tensor on GPU using `to(device)` (e.g. `<tensor_name>.to(device)`) returns a copy of that tensor, e.g. the same tensor will be on CPU and GPU. To overwrite tensors, reassign them:\n",
    "  `<tensor_name> = <tensor_name>.to(device)`\n",
    "- Tensor will move *CPU* to *GPU* if *GPU* is available else the tensor will be present in *CPU*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b66d4e-b363-4409-ba18-836888ad2c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "device :  cpu\n",
      "tensor([1, 2, 3])\n",
      "device :  cpu\n"
     ]
    }
   ],
   "source": [
    "# Create tensor (default on CPU)\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Tensor not on GPU\n",
    "print(tensor)\n",
    "print(\"device : \", tensor.device)\n",
    "\n",
    "# Move tensor to GPU (if available)\n",
    "tensor_to_gpu = tensor.to(device)\n",
    "print(tensor_to_gpu)\n",
    "print(\"device : \", tensor_to_gpu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba223971-ea6b-4fd3-85ea-5e5a4e03aa89",
   "metadata": {},
   "source": [
    "### **4. Moving tensors back to the CPU**\n",
    "- `GPU` >---`Tensor.cpu()`--->>> `CPU`\n",
    "- Move the tensor back to CPU.\n",
    "- For example, you'll want to do this if you want to interact with your tensors with NumPy (NumPy does not leverage the GPU).\n",
    "- Let's try using the `torch.Tensor.numpy()` method on our `tensor_on_gpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a57f4c72-8b54-43ce-93c9-8a8d8daf8e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If tensor is on GPU, can't transform it to NumPy (this will error)\n",
    "# tensor_on_gpu.numpy()   # error\n",
    "tensor_on_gpu = tensor.cpu()\n",
    "tensor_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ebc34-dad3-4fe4-aa8e-d086257700ba",
   "metadata": {},
   "source": [
    "- Instead, to get a tensor back to CPU and usable with NumPy we can use `Tensor.cpu()`. This copies the tensor to CPU memory so it's usable with CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8191758-3b6c-4ec3-94e4-250dbb8a2e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead, copy the tensor back to cpu\n",
    "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()    # returns a copys of the GPU tensor in CPU memory so the original tensor is still on GPU.\n",
    "tensor_back_on_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dfa5b5-2ccb-4468-b814-0fc4d1244d03",
   "metadata": {},
   "source": [
    "- The above returns a copys of the GPU tensor in CPU memory so the original tensor is still on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359bebd3-4c4e-4db3-9a38-19ea77588e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0321a1f-b880-49cb-96d4-af809c1d01ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
